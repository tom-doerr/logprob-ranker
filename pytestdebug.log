versions pytest-7.3.1, python-3.8.10.final.0
cwd=/home/tom/git/logprob-ranker
args=('--debug',)

  pytest_cmdline_main [hook]
      config: <_pytest.config.Config object at 0x7f645106a280>
    pytest_plugin_registered [hook]
        plugin: <Session logprob-ranker exitstatus='<UNSET>' testsfailed=0 testscollected=0>
        manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
    finish pytest_plugin_registered --> [] [hook]
    pytest_configure [hook]
        config: <_pytest.config.Config object at 0x7f645106a280>
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x7f6450ac3cd0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x7f6450ac3310>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: faulthandler [assertion]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: pdb [assertion]
    early skip of rewriting module: cmd [assertion]
    early skip of rewriting module: code [assertion]
    early skip of rewriting module: codeop [assertion]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x7f645106a280>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/usr/local/lib/python3.8/dist-packages/_pytest/mark/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/usr/local/lib/python3.8/dist-packages/_pytest/main.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/usr/local/lib/python3.8/dist-packages/_pytest/runner.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/usr/local/lib/python3.8/dist-packages/_pytest/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/usr/local/lib/python3.8/dist-packages/_pytest/helpconfig.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/usr/local/lib/python3.8/dist-packages/_pytest/python.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/usr/local/lib/python3.8/dist-packages/_pytest/terminal.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/usr/local/lib/python3.8/dist-packages/_pytest/debugging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/usr/local/lib/python3.8/dist-packages/_pytest/unittest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/usr/local/lib/python3.8/dist-packages/_pytest/capture.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/usr/local/lib/python3.8/dist-packages/_pytest/skipping.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/usr/local/lib/python3.8/dist-packages/_pytest/legacypath.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/usr/local/lib/python3.8/dist-packages/_pytest/tmpdir.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/usr/local/lib/python3.8/dist-packages/_pytest/monkeypatch.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/usr/local/lib/python3.8/dist-packages/_pytest/recwarn.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/usr/local/lib/python3.8/dist-packages/_pytest/pastebin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.nose' from '/usr/local/lib/python3.8/dist-packages/_pytest/nose.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/usr/local/lib/python3.8/dist-packages/_pytest/junitxml.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/usr/local/lib/python3.8/dist-packages/_pytest/doctest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/usr/local/lib/python3.8/dist-packages/_pytest/cacheprovider.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/usr/local/lib/python3.8/dist-packages/_pytest/freeze_support.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/usr/local/lib/python3.8/dist-packages/_pytest/setuponly.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/usr/local/lib/python3.8/dist-packages/_pytest/setupplan.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/usr/local/lib/python3.8/dist-packages/_pytest/stepwise.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/usr/local/lib/python3.8/dist-packages/_pytest/warnings.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/usr/local/lib/python3.8/dist-packages/_pytest/logging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/usr/local/lib/python3.8/dist-packages/_pytest/reports.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/usr/local/lib/python3.8/dist-packages/_pytest/python_path.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/usr/local/lib/python3.8/dist-packages/_pytest/unraisableexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/usr/local/lib/python3.8/dist-packages/_pytest/threadexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/usr/local/lib/python3.8/dist-packages/_pytest/faulthandler.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_mock' from '/home/tom/.local/lib/python3.8/site-packages/pytest_mock/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'typeguard.pytest_plugin' from '/home/tom/.local/lib/python3.8/site-packages/typeguard/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'anyio.pytest_plugin' from '/home/tom/.local/lib/python3.8/site-packages/anyio/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_cov.plugin' from '/home/tom/.local/lib/python3.8/site-packages/pytest_cov/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x7f6450ac3cd0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x7f6450ac3310>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x7f6450abb850>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x7f6450abb6d0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
    finish pytest_configure --> [] [hook]
    pytest_sessionstart [hook]
        session: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x7f645106a280>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/usr/local/lib/python3.8/dist-packages/_pytest/mark/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/usr/local/lib/python3.8/dist-packages/_pytest/main.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/usr/local/lib/python3.8/dist-packages/_pytest/runner.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/usr/local/lib/python3.8/dist-packages/_pytest/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/usr/local/lib/python3.8/dist-packages/_pytest/helpconfig.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/usr/local/lib/python3.8/dist-packages/_pytest/python.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/usr/local/lib/python3.8/dist-packages/_pytest/terminal.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/usr/local/lib/python3.8/dist-packages/_pytest/debugging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/usr/local/lib/python3.8/dist-packages/_pytest/unittest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/usr/local/lib/python3.8/dist-packages/_pytest/capture.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/usr/local/lib/python3.8/dist-packages/_pytest/skipping.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/usr/local/lib/python3.8/dist-packages/_pytest/legacypath.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/usr/local/lib/python3.8/dist-packages/_pytest/tmpdir.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/usr/local/lib/python3.8/dist-packages/_pytest/monkeypatch.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/usr/local/lib/python3.8/dist-packages/_pytest/recwarn.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/usr/local/lib/python3.8/dist-packages/_pytest/pastebin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.nose' from '/usr/local/lib/python3.8/dist-packages/_pytest/nose.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/usr/local/lib/python3.8/dist-packages/_pytest/junitxml.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/usr/local/lib/python3.8/dist-packages/_pytest/doctest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/usr/local/lib/python3.8/dist-packages/_pytest/cacheprovider.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/usr/local/lib/python3.8/dist-packages/_pytest/freeze_support.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/usr/local/lib/python3.8/dist-packages/_pytest/setuponly.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/usr/local/lib/python3.8/dist-packages/_pytest/setupplan.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/usr/local/lib/python3.8/dist-packages/_pytest/stepwise.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/usr/local/lib/python3.8/dist-packages/_pytest/warnings.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/usr/local/lib/python3.8/dist-packages/_pytest/logging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/usr/local/lib/python3.8/dist-packages/_pytest/reports.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/usr/local/lib/python3.8/dist-packages/_pytest/python_path.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/usr/local/lib/python3.8/dist-packages/_pytest/unraisableexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/usr/local/lib/python3.8/dist-packages/_pytest/threadexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/usr/local/lib/python3.8/dist-packages/_pytest/faulthandler.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_mock' from '/home/tom/.local/lib/python3.8/site-packages/pytest_mock/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'typeguard.pytest_plugin' from '/home/tom/.local/lib/python3.8/site-packages/typeguard/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'anyio.pytest_plugin' from '/home/tom/.local/lib/python3.8/site-packages/anyio/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_cov.plugin' from '/home/tom/.local/lib/python3.8/site-packages/pytest_cov/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x7f6450ac3cd0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x7f6450ac3310>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x7f6450abb850>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x7f6450abb6d0>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.fixtures.FixtureManager object at 0x7f6450ac3340>
          manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
      finish pytest_plugin_registered --> [] [hook]
      pytest_report_header [hook]
          config: <_pytest.config.Config object at 0x7f645106a280>
          start_path: /home/tom/git/logprob-ranker
          startdir: /home/tom/git/logprob-ranker
      early skip of rewriting module: email.parser [assertion]
      early skip of rewriting module: email.feedparser [assertion]
      early skip of rewriting module: email.errors [assertion]
      early skip of rewriting module: email._policybase [assertion]
      early skip of rewriting module: email.header [assertion]
      early skip of rewriting module: email.quoprimime [assertion]
      early skip of rewriting module: email.base64mime [assertion]
      early skip of rewriting module: email.charset [assertion]
      early skip of rewriting module: email.encoders [assertion]
      early skip of rewriting module: quopri [assertion]
      early skip of rewriting module: email.utils [assertion]
      early skip of rewriting module: email._parseaddr [assertion]
      early skip of rewriting module: calendar [assertion]
      early skip of rewriting module: email.message [assertion]
      early skip of rewriting module: uu [assertion]
      early skip of rewriting module: email._encoded_words [assertion]
      early skip of rewriting module: email.iterators [assertion]
      finish pytest_report_header --> [['rootdir: /home/tom/git/logprob-ranker', 'configfile: pytest.ini', 'plugins: mock-3.14.0, typeguard-2.13.3, anyio-4.5.2, cov-5.0.0'], ['using: pytest-7.3.1', 'setuptools registered plugins:', '  pytest-mock-3.14.0 at /home/tom/.local/lib/python3.8/site-packages/pytest_mock/__init__.py', '  typeguard-2.13.3 at /home/tom/.local/lib/python3.8/site-packages/typeguard/pytest_plugin.py', '  anyio-4.5.2 at /home/tom/.local/lib/python3.8/site-packages/anyio/pytest_plugin.py', '  pytest-cov-5.0.0 at /home/tom/.local/lib/python3.8/site-packages/pytest_cov/plugin.py']] [hook]
    finish pytest_sessionstart --> [] [hook]
    pytest_collection [hook]
        session: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
    perform_collect <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0> ['/home/tom/git/logprob-ranker'] [collection]
        pytest_collectstart [hook]
            collector: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        finish pytest_collectstart --> [] [hook]
        pytest_make_collect_report [hook]
            collector: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        processing argument (PosixPath('/home/tom/git/logprob-ranker'), []) [collection]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/.gitignore
                path: /home/tom/git/logprob-ranker/.gitignore
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/.gitignore
                path: /home/tom/git/logprob-ranker/.gitignore
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/.pylintrc
                path: /home/tom/git/logprob-ranker/.pylintrc
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/.pylintrc
                path: /home/tom/git/logprob-ranker/.pylintrc
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/README.md
                path: /home/tom/git/logprob-ranker/README.md
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/README.md
                path: /home/tom/git/logprob-ranker/README.md
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/check_env.py
                path: /home/tom/git/logprob-ranker/check_env.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/check_env.py
                path: /home/tom/git/logprob-ranker/check_env.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/example.py
                path: /home/tom/git/logprob-ranker/example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/example.py
                path: /home/tom/git/logprob-ranker/example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/info.md
                path: /home/tom/git/logprob-ranker/info.md
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/info.md
                path: /home/tom/git/logprob-ranker/info.md
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/pyproject.toml
                path: /home/tom/git/logprob-ranker/pyproject.toml
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/pyproject.toml
                path: /home/tom/git/logprob-ranker/pyproject.toml
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/pytest.ini
                path: /home/tom/git/logprob-ranker/pytest.ini
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/pytest.ini
                path: /home/tom/git/logprob-ranker/pytest.ini
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/pytestdebug.log
                path: /home/tom/git/logprob-ranker/pytestdebug.log
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/pytestdebug.log
                path: /home/tom/git/logprob-ranker/pytestdebug.log
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/.git
                path: /home/tom/git/logprob-ranker/.git
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/.pytest_cache
                path: /home/tom/git/logprob-ranker/.pytest_cache
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/.venv
                path: /home/tom/git/logprob-ranker/.venv
            finish pytest_ignore_collect --> True [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker
                path: /home/tom/git/logprob-ranker/logprob_ranker
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
                  path: /home/tom/git/logprob-ranker/logprob_ranker/__init__.py
              finish pytest_pycollect_makemodule --> <Package logprob_ranker> [hook]
            finish pytest_collect_file --> [<Package logprob_ranker>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.pytest_cache
                path: /home/tom/git/logprob-ranker/logprob_ranker/.pytest_cache
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.venv
                path: /home/tom/git/logprob-ranker/logprob_ranker/.venv
            finish pytest_ignore_collect --> True [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/build
                path: /home/tom/git/logprob-ranker/logprob_ranker/build
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs
                path: /home/tom/git/logprob-ranker/logprob_ranker/docs
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
                path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
                path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
                path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
                path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
                path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
                  path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
              finish pytest_pycollect_makemodule --> <Package logprob_ranker> [hook]
            finish pytest_collect_file --> [<Package logprob_ranker>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
                path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests
                path: /home/tom/git/logprob-ranker/logprob_ranker/tests
            finish pytest_ignore_collect --> None [hook]
          early skip of rewriting module: logprob_ranker [assertion]
          early skip of rewriting module: logprob_ranker.logprob_ranker [assertion]
          early skip of rewriting module: logprob_ranker.logprob_ranker.ranker [assertion]
          early skip of rewriting module: aiohttp [assertion]
          early skip of rewriting module: aiohttp.hdrs [assertion]
          early skip of rewriting module: multidict [assertion]
          early skip of rewriting module: multidict._abc [assertion]
          early skip of rewriting module: multidict._compat [assertion]
          early skip of rewriting module: multidict._multidict [assertion]
          early skip of rewriting module: multidict._multidict_base [assertion]
          early skip of rewriting module: aiohttp.client [assertion]
          early skip of rewriting module: attr [assertion]
          early skip of rewriting module: attr.converters [assertion]
          early skip of rewriting module: attr._compat [assertion]
          early skip of rewriting module: attr._make [assertion]
          early skip of rewriting module: attr._config [assertion]
          early skip of rewriting module: attr.setters [assertion]
          early skip of rewriting module: attr.exceptions [assertion]
          early skip of rewriting module: attr.filters [assertion]
          early skip of rewriting module: attr.validators [assertion]
          early skip of rewriting module: attr._cmp [assertion]
          early skip of rewriting module: attr._funcs [assertion]
          early skip of rewriting module: attr._next_gen [assertion]
          early skip of rewriting module: attr._version_info [assertion]
          early skip of rewriting module: yarl [assertion]
          early skip of rewriting module: yarl._url [assertion]
          early skip of rewriting module: idna [assertion]
          early skip of rewriting module: idna.core [assertion]
          early skip of rewriting module: idna.idnadata [assertion]
          early skip of rewriting module: idna.intranges [assertion]
          early skip of rewriting module: idna.package_data [assertion]
          early skip of rewriting module: yarl._quoting [assertion]
          early skip of rewriting module: yarl._quoting_c [assertion]
          early skip of rewriting module: aiohttp.http [assertion]
          early skip of rewriting module: http [assertion]
          early skip of rewriting module: http.server [assertion]
          early skip of rewriting module: html [assertion]
          early skip of rewriting module: html.entities [assertion]
          early skip of rewriting module: http.client [assertion]
          early skip of rewriting module: mimetypes [assertion]
          early skip of rewriting module: winreg [assertion]
          early skip of rewriting module: socketserver [assertion]
          early skip of rewriting module: aiohttp.http_exceptions [assertion]
          early skip of rewriting module: aiohttp.typedefs [assertion]
          early skip of rewriting module: aiohttp.http_parser [assertion]
          early skip of rewriting module: aiohttp.base_protocol [assertion]
          early skip of rewriting module: aiohttp.tcp_helpers [assertion]
          early skip of rewriting module: aiohttp.helpers [assertion]
          early skip of rewriting module: netrc [assertion]
          early skip of rewriting module: urllib.request [assertion]
          early skip of rewriting module: urllib.error [assertion]
          early skip of rewriting module: urllib.response [assertion]
          early skip of rewriting module: async_timeout [assertion]
          early skip of rewriting module: aiohttp.log [assertion]
          early skip of rewriting module: aiohttp._helpers [assertion]
          early skip of rewriting module: aiohttp.http_writer [assertion]
          early skip of rewriting module: aiohttp.abc [assertion]
          early skip of rewriting module: http.cookies [assertion]
          early skip of rewriting module: aiohttp._http_writer [assertion]
          early skip of rewriting module: aiohttp.streams [assertion]
          early skip of rewriting module: brotli [assertion]
          early skip of rewriting module: _brotli [assertion]
          early skip of rewriting module: aiohttp._http_parser [assertion]
          early skip of rewriting module: backports_abc [assertion]
          early skip of rewriting module: aiohttp.http_websocket [assertion]
          early skip of rewriting module: aiohttp._websocket [assertion]
          early skip of rewriting module: aiohttp.payload [assertion]
          early skip of rewriting module: aiohttp.client_exceptions [assertion]
          early skip of rewriting module: aiohttp.client_reqrep [assertion]
          early skip of rewriting module: aiohttp.multipart [assertion]
          early skip of rewriting module: aiohttp.formdata [assertion]
          early skip of rewriting module: cchardet [assertion]
          early skip of rewriting module: charset_normalizer [assertion]
          early skip of rewriting module: charset_normalizer.api [assertion]
          early skip of rewriting module: charset_normalizer.cd [assertion]
          early skip of rewriting module: charset_normalizer.constant [assertion]
          early skip of rewriting module: charset_normalizer.md [assertion]
          early skip of rewriting module: charset_normalizer.md__mypyc [assertion]
          early skip of rewriting module: charset_normalizer.utils [assertion]
          early skip of rewriting module: _multibytecodec [assertion]
          early skip of rewriting module: charset_normalizer.models [assertion]
          early skip of rewriting module: charset_normalizer.legacy [assertion]
          early skip of rewriting module: charset_normalizer.version [assertion]
          early skip of rewriting module: aiohttp.client_ws [assertion]
          early skip of rewriting module: aiohttp.connector [assertion]
          early skip of rewriting module: aiohttp.client_proto [assertion]
          early skip of rewriting module: aiohttp.locks [assertion]
          early skip of rewriting module: aiohttp.resolver [assertion]
          early skip of rewriting module: aiodns [assertion]
          early skip of rewriting module: aiohttp.cookiejar [assertion]
          early skip of rewriting module: aiohttp.tracing [assertion]
          early skip of rewriting module: aiosignal [assertion]
          early skip of rewriting module: frozenlist [assertion]
          early skip of rewriting module: frozenlist._frozenlist [assertion]
          early skip of rewriting module: aiohttp.payload_streamer [assertion]
          early skip of rewriting module: aiohttp.worker [assertion]
          early skip of rewriting module: gunicorn [assertion]
          early skip of rewriting module: gunicorn.config [assertion]
          early skip of rewriting module: gunicorn.util [assertion]
          early skip of rewriting module: fcntl [assertion]
          early skip of rewriting module: gunicorn.errors [assertion]
          early skip of rewriting module: gunicorn.workers [assertion]
          early skip of rewriting module: setproctitle [assertion]
          early skip of rewriting module: gunicorn.reloader [assertion]
          early skip of rewriting module: inotify [assertion]
          early skip of rewriting module: gunicorn.workers.base [assertion]
          early skip of rewriting module: gunicorn.http [assertion]
          early skip of rewriting module: gunicorn.http.message [assertion]
          early skip of rewriting module: gunicorn.http.body [assertion]
          early skip of rewriting module: gunicorn.http.errors [assertion]
          early skip of rewriting module: gunicorn.http.parser [assertion]
          early skip of rewriting module: gunicorn.http.unreader [assertion]
          early skip of rewriting module: gunicorn.http.wsgi [assertion]
          early skip of rewriting module: gunicorn.workers.workertmp [assertion]
          early skip of rewriting module: aiohttp.web [assertion]
          early skip of rewriting module: aiohttp.web_app [assertion]
          early skip of rewriting module: aiohttp.web_log [assertion]
          early skip of rewriting module: aiohttp.web_request [assertion]
          early skip of rewriting module: aiohttp.web_exceptions [assertion]
          early skip of rewriting module: aiohttp.web_response [assertion]
          early skip of rewriting module: aiohttp.web_middlewares [assertion]
          early skip of rewriting module: aiohttp.web_urldispatcher [assertion]
          early skip of rewriting module: aiohttp.web_fileresponse [assertion]
          early skip of rewriting module: aiohttp.web_routedef [assertion]
          early skip of rewriting module: aiohttp.web_protocol [assertion]
          early skip of rewriting module: aiohttp.web_server [assertion]
          early skip of rewriting module: aiohttp.web_runner [assertion]
          early skip of rewriting module: aiohttp.web_ws [assertion]
          early skip of rewriting module: litellm [assertion]
          early skip of rewriting module: litellm.llms [assertion]
          early skip of rewriting module: litellm.llms.custom_httpx [assertion]
          early skip of rewriting module: litellm.llms.custom_httpx.http_handler [assertion]
          early skip of rewriting module: httpx [assertion]
          early skip of rewriting module: httpx.__version__ [assertion]
          early skip of rewriting module: httpx._api [assertion]
          early skip of rewriting module: httpx._client [assertion]
          early skip of rewriting module: httpx._auth [assertion]
          early skip of rewriting module: httpx._exceptions [assertion]
          early skip of rewriting module: httpx._models [assertion]
          early skip of rewriting module: http.cookiejar [assertion]
          early skip of rewriting module: httpx._content [assertion]
          early skip of rewriting module: httpx._multipart [assertion]
          early skip of rewriting module: httpx._types [assertion]
          early skip of rewriting module: httpx._utils [assertion]
          early skip of rewriting module: httpx._decoders [assertion]
          early skip of rewriting module: zstandard [assertion]
          early skip of rewriting module: httpx._status_codes [assertion]
          early skip of rewriting module: httpx._urls [assertion]
          early skip of rewriting module: httpx._urlparse [assertion]
          early skip of rewriting module: httpx._config [assertion]
          early skip of rewriting module: httpx._transports [assertion]
          early skip of rewriting module: httpx._transports.asgi [assertion]
          early skip of rewriting module: httpx._transports.base [assertion]
          early skip of rewriting module: httpx._transports.default [assertion]
          early skip of rewriting module: httpx._transports.mock [assertion]
          early skip of rewriting module: httpx._transports.wsgi [assertion]
          early skip of rewriting module: httpx._main [assertion]
          early skip of rewriting module: click [assertion]
          early skip of rewriting module: click.core [assertion]
          early skip of rewriting module: click.types [assertion]
          early skip of rewriting module: click._compat [assertion]
          early skip of rewriting module: click.exceptions [assertion]
          early skip of rewriting module: click.globals [assertion]
          early skip of rewriting module: click.utils [assertion]
          early skip of rewriting module: click.formatting [assertion]
          early skip of rewriting module: click.parser [assertion]
          early skip of rewriting module: click.termui [assertion]
          early skip of rewriting module: click.decorators [assertion]
          early skip of rewriting module: pygments [assertion]
          early skip of rewriting module: pygments.lexers [assertion]
          early skip of rewriting module: pygments.lexers._mapping [assertion]
          early skip of rewriting module: pygments.modeline [assertion]
          early skip of rewriting module: pygments.plugin [assertion]
          early skip of rewriting module: pygments.util [assertion]
          early skip of rewriting module: rich [assertion]
          early skip of rewriting module: rich._extension [assertion]
          early skip of rewriting module: rich.console [assertion]
          early skip of rewriting module: getpass [assertion]
          early skip of rewriting module: termios [assertion]
          early skip of rewriting module: rich._null_file [assertion]
          early skip of rewriting module: rich.errors [assertion]
          early skip of rewriting module: rich.themes [assertion]
          early skip of rewriting module: rich.default_styles [assertion]
          early skip of rewriting module: rich.style [assertion]
          early skip of rewriting module: rich.color [assertion]
          early skip of rewriting module: colorsys [assertion]
          early skip of rewriting module: rich._palettes [assertion]
          early skip of rewriting module: rich.palette [assertion]
          early skip of rewriting module: rich.color_triplet [assertion]
          early skip of rewriting module: rich.repr [assertion]
          early skip of rewriting module: rich.terminal_theme [assertion]
          early skip of rewriting module: rich.theme [assertion]
          early skip of rewriting module: rich._emoji_replace [assertion]
          early skip of rewriting module: rich._emoji_codes [assertion]
          early skip of rewriting module: rich._export_format [assertion]
          early skip of rewriting module: rich._fileno [assertion]
          early skip of rewriting module: rich._log_render [assertion]
          early skip of rewriting module: rich.text [assertion]
          early skip of rewriting module: rich._loop [assertion]
          early skip of rewriting module: rich._pick [assertion]
          early skip of rewriting module: rich._wrap [assertion]
          early skip of rewriting module: rich.cells [assertion]
          early skip of rewriting module: rich._cell_widths [assertion]
          early skip of rewriting module: rich.align [assertion]
          early skip of rewriting module: rich.constrain [assertion]
          early skip of rewriting module: rich.jupyter [assertion]
          early skip of rewriting module: rich.segment [assertion]
          early skip of rewriting module: rich.measure [assertion]
          early skip of rewriting module: rich.protocol [assertion]
          early skip of rewriting module: rich.containers [assertion]
          early skip of rewriting module: rich.control [assertion]
          early skip of rewriting module: rich.emoji [assertion]
          early skip of rewriting module: rich.highlighter [assertion]
          early skip of rewriting module: rich.markup [assertion]
          early skip of rewriting module: rich.pager [assertion]
          early skip of rewriting module: rich.pretty [assertion]
          early skip of rewriting module: rich.abc [assertion]
          early skip of rewriting module: rich.region [assertion]
          early skip of rewriting module: rich.scope [assertion]
          early skip of rewriting module: rich.panel [assertion]
          early skip of rewriting module: rich.box [assertion]
          early skip of rewriting module: rich.padding [assertion]
          early skip of rewriting module: rich.table [assertion]
          early skip of rewriting module: rich._ratio [assertion]
          early skip of rewriting module: fractions [assertion]
          early skip of rewriting module: rich.screen [assertion]
          early skip of rewriting module: rich.styled [assertion]
          early skip of rewriting module: rich.progress [assertion]
          early skip of rewriting module: mmap [assertion]
          early skip of rewriting module: rich.filesize [assertion]
          early skip of rewriting module: rich.live [assertion]
          early skip of rewriting module: rich.file_proxy [assertion]
          early skip of rewriting module: rich.ansi [assertion]
          early skip of rewriting module: rich.live_render [assertion]
          early skip of rewriting module: rich.progress_bar [assertion]
          early skip of rewriting module: rich.spinner [assertion]
          early skip of rewriting module: rich._spinners [assertion]
          early skip of rewriting module: rich.syntax [assertion]
          early skip of rewriting module: pygments.lexer [assertion]
          early skip of rewriting module: pygments.filter [assertion]
          early skip of rewriting module: pygments.filters [assertion]
          early skip of rewriting module: pygments.token [assertion]
          early skip of rewriting module: pygments.regexopt [assertion]
          early skip of rewriting module: pygments.style [assertion]
          early skip of rewriting module: pygments.styles [assertion]
          early skip of rewriting module: litellm.litellm_core_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.logging_utils [assertion]
          early skip of rewriting module: litellm._logging [assertion]
          early skip of rewriting module: litellm.types [assertion]
          early skip of rewriting module: litellm.types.utils [assertion]
          early skip of rewriting module: openai [assertion]
          early skip of rewriting module: openai.types [assertion]
          early skip of rewriting module: openai.types.batch [assertion]
          early skip of rewriting module: openai._models [assertion]
          early skip of rewriting module: pydantic [assertion]
          early skip of rewriting module: pydantic._migration [assertion]
          early skip of rewriting module: pydantic.version [assertion]
          early skip of rewriting module: pydantic.errors [assertion]
          early skip of rewriting module: pydantic.generics [assertion]
          early skip of rewriting module: pydantic.fields [assertion]
          early skip of rewriting module: annotated_types [assertion]
          early skip of rewriting module: pydantic_core [assertion]
          early skip of rewriting module: pydantic_core._pydantic_core [assertion]
          early skip of rewriting module: pydantic_core.core_schema [assertion]
          early skip of rewriting module: pydantic._internal [assertion]
          early skip of rewriting module: pydantic._internal._validators [assertion]
          early skip of rewriting module: pydantic.types [assertion]
          early skip of rewriting module: pydantic._internal._core_utils [assertion]
          early skip of rewriting module: pydantic._internal._repr [assertion]
          early skip of rewriting module: pydantic._internal._typing_extra [assertion]
          early skip of rewriting module: pydantic._internal._namespace_utils [assertion]
          early skip of rewriting module: pydantic._internal._core_metadata [assertion]
          early skip of rewriting module: pydantic._internal._fields [assertion]
          early skip of rewriting module: pydantic._internal._config [assertion]
          early skip of rewriting module: pydantic.aliases [assertion]
          early skip of rewriting module: pydantic._internal._internal_dataclass [assertion]
          early skip of rewriting module: pydantic.config [assertion]
          early skip of rewriting module: pydantic.warnings [assertion]
          early skip of rewriting module: pydantic._internal._docs_extraction [assertion]
          early skip of rewriting module: pydantic._internal._import_utils [assertion]
          early skip of rewriting module: pydantic._internal._utils [assertion]
          early skip of rewriting module: pydantic.annotated_handlers [assertion]
          early skip of rewriting module: pydantic.json_schema [assertion]
          early skip of rewriting module: pydantic._internal._decorators [assertion]
          early skip of rewriting module: pydantic._internal._mock_val_ser [assertion]
          early skip of rewriting module: pydantic.plugin [assertion]
          early skip of rewriting module: pydantic.plugin._schema_validator [assertion]
          early skip of rewriting module: pydantic._internal._schema_generation_shared [assertion]
          early skip of rewriting module: pydantic._internal._generics [assertion]
          early skip of rewriting module: pydantic._internal._forward_ref [assertion]
          early skip of rewriting module: openai._types [assertion]
          early skip of rewriting module: pydantic.main [assertion]
          early skip of rewriting module: pydantic._internal._model_construction [assertion]
          early skip of rewriting module: pydantic._internal._generate_schema [assertion]
          early skip of rewriting module: pydantic.functional_validators [assertion]
          early skip of rewriting module: pydantic._internal._discriminated_union [assertion]
          early skip of rewriting module: pydantic._internal._known_annotated_metadata [assertion]
          early skip of rewriting module: pydantic._internal._signature [assertion]
          early skip of rewriting module: openai._utils [assertion]
          early skip of rewriting module: openai._utils._logs [assertion]
          early skip of rewriting module: openai._utils._utils [assertion]
          early skip of rewriting module: openai._compat [assertion]
          early skip of rewriting module: pydantic.v1 [assertion]
          early skip of rewriting module: pydantic.v1.dataclasses [assertion]
          early skip of rewriting module: pydantic.v1.class_validators [assertion]
          early skip of rewriting module: pydantic.v1.errors [assertion]
          early skip of rewriting module: pydantic.v1.typing [assertion]
          early skip of rewriting module: pydantic.v1.utils [assertion]
          early skip of rewriting module: pydantic.v1.version [assertion]
          early skip of rewriting module: cython [assertion]
          early skip of rewriting module: Cython [assertion]
          early skip of rewriting module: Cython.Shadow [assertion]
          early skip of rewriting module: __builtin__ [assertion]
          early skip of rewriting module: __builtin__ [assertion]
          early skip of rewriting module: pydantic.v1.config [assertion]
          early skip of rewriting module: pydantic.v1.error_wrappers [assertion]
          early skip of rewriting module: pydantic.v1.json [assertion]
          early skip of rewriting module: pydantic.v1.color [assertion]
          early skip of rewriting module: pydantic.v1.networks [assertion]
          early skip of rewriting module: pydantic.v1.validators [assertion]
          early skip of rewriting module: pydantic.v1.datetime_parse [assertion]
          early skip of rewriting module: pydantic.v1.types [assertion]
          early skip of rewriting module: pydantic.v1.fields [assertion]
          early skip of rewriting module: pydantic.v1.main [assertion]
          early skip of rewriting module: pydantic.v1.parse [assertion]
          early skip of rewriting module: pydantic.v1.schema [assertion]
          early skip of rewriting module: pydantic.v1.annotated_types [assertion]
          early skip of rewriting module: pydantic.v1.decorator [assertion]
          early skip of rewriting module: pydantic.v1.env_settings [assertion]
          early skip of rewriting module: pydantic.v1.tools [assertion]
          early skip of rewriting module: pydantic.plugin._loader [assertion]
          early skip of rewriting module: openai._utils._sync [assertion]
          early skip of rewriting module: openai._utils._proxy [assertion]
          early skip of rewriting module: openai._utils._typing [assertion]
          early skip of rewriting module: openai._utils._streams [assertion]
          early skip of rewriting module: openai._utils._transform [assertion]
          early skip of rewriting module: openai._files [assertion]
          early skip of rewriting module: openai._utils._reflection [assertion]
          early skip of rewriting module: openai._constants [assertion]
          early skip of rewriting module: pydantic.type_adapter [assertion]
          early skip of rewriting module: pydantic._internal._std_types_schema [assertion]
          early skip of rewriting module: pydantic._internal._serializers [assertion]
          early skip of rewriting module: openai.types.batch_error [assertion]
          early skip of rewriting module: openai.types.shared [assertion]
          early skip of rewriting module: openai.types.shared.metadata [assertion]
          early skip of rewriting module: openai.types.shared.reasoning [assertion]
          early skip of rewriting module: openai.types.shared.reasoning_effort [assertion]
          early skip of rewriting module: openai.types.shared.all_models [assertion]
          early skip of rewriting module: openai.types.shared.chat_model [assertion]
          early skip of rewriting module: openai.types.shared.error_object [assertion]
          early skip of rewriting module: openai.types.shared.compound_filter [assertion]
          early skip of rewriting module: openai.types.shared.comparison_filter [assertion]
          early skip of rewriting module: openai.types.shared.responses_model [assertion]
          early skip of rewriting module: openai.types.shared.function_definition [assertion]
          early skip of rewriting module: openai.types.shared.function_parameters [assertion]
          early skip of rewriting module: openai.types.shared.response_format_text [assertion]
          early skip of rewriting module: openai.types.shared.response_format_json_object [assertion]
          early skip of rewriting module: openai.types.shared.response_format_json_schema [assertion]
          early skip of rewriting module: openai.types.batch_request_counts [assertion]
          early skip of rewriting module: openai.types.image [assertion]
          early skip of rewriting module: openai.types.model [assertion]
          early skip of rewriting module: openai.types.upload [assertion]
          early skip of rewriting module: openai.types.file_object [assertion]
          early skip of rewriting module: openai.types.embedding [assertion]
          early skip of rewriting module: openai.types.chat_model [assertion]
          early skip of rewriting module: openai.types.completion [assertion]
          early skip of rewriting module: openai.types.completion_usage [assertion]
          early skip of rewriting module: openai.types.completion_choice [assertion]
          early skip of rewriting module: openai.types.moderation [assertion]
          early skip of rewriting module: openai.types.audio_model [assertion]
          early skip of rewriting module: openai.types.image_model [assertion]
          early skip of rewriting module: openai.types.file_content [assertion]
          early skip of rewriting module: openai.types.file_deleted [assertion]
          early skip of rewriting module: openai.types.file_purpose [assertion]
          early skip of rewriting module: openai.types.vector_store [assertion]
          early skip of rewriting module: openai.types.model_deleted [assertion]
          early skip of rewriting module: openai.types.embedding_model [assertion]
          early skip of rewriting module: openai.types.images_response [assertion]
          early skip of rewriting module: openai.types.eval_list_params [assertion]
          early skip of rewriting module: openai.types.file_list_params [assertion]
          early skip of rewriting module: openai.types.moderation_model [assertion]
          early skip of rewriting module: openai.types.batch_list_params [assertion]
          early skip of rewriting module: openai.types.image_edit_params [assertion]
          early skip of rewriting module: openai.types.eval_create_params [assertion]
          early skip of rewriting module: openai.types.shared_params [assertion]
          early skip of rewriting module: openai.types.shared_params.metadata [assertion]
          early skip of rewriting module: openai.types.shared_params.reasoning [assertion]
          early skip of rewriting module: openai.types.shared_params.chat_model [assertion]
          early skip of rewriting module: openai.types.shared_params.compound_filter [assertion]
          early skip of rewriting module: openai.types.shared_params.comparison_filter [assertion]
          early skip of rewriting module: openai.types.shared_params.responses_model [assertion]
          early skip of rewriting module: openai.types.shared_params.reasoning_effort [assertion]
          early skip of rewriting module: openai.types.shared_params.function_definition [assertion]
          early skip of rewriting module: openai.types.shared_params.function_parameters [assertion]
          early skip of rewriting module: openai.types.shared_params.response_format_text [assertion]
          early skip of rewriting module: openai.types.shared_params.response_format_json_object [assertion]
          early skip of rewriting module: openai.types.shared_params.response_format_json_schema [assertion]
          early skip of rewriting module: openai.types.eval_string_check_grader_param [assertion]
          early skip of rewriting module: openai.types.eval_text_similarity_grader_param [assertion]
          early skip of rewriting module: openai.types.eval_list_response [assertion]
          early skip of rewriting module: openai.types.eval_label_model_grader [assertion]
          early skip of rewriting module: openai.types.eval_string_check_grader [assertion]
          early skip of rewriting module: openai.types.eval_text_similarity_grader [assertion]
          early skip of rewriting module: openai.types.eval_custom_data_source_config [assertion]
          early skip of rewriting module: openai.types.eval_stored_completions_data_source_config [assertion]
          early skip of rewriting module: openai.types.eval_update_params [assertion]
          early skip of rewriting module: openai.types.file_create_params [assertion]
          early skip of rewriting module: openai.types.batch_create_params [assertion]
          early skip of rewriting module: openai.types.eval_create_response [assertion]
          early skip of rewriting module: openai.types.eval_delete_response [assertion]
          early skip of rewriting module: openai.types.eval_update_response [assertion]
          early skip of rewriting module: openai.types.upload_create_params [assertion]
          early skip of rewriting module: openai.types.vector_store_deleted [assertion]
          early skip of rewriting module: openai.types.audio_response_format [assertion]
          early skip of rewriting module: openai.types.image_generate_params [assertion]
          early skip of rewriting module: openai.types.eval_retrieve_response [assertion]
          early skip of rewriting module: openai.types.file_chunking_strategy [assertion]
          early skip of rewriting module: openai.types.other_file_chunking_strategy_object [assertion]
          early skip of rewriting module: openai.types.static_file_chunking_strategy_object [assertion]
          early skip of rewriting module: openai.types.static_file_chunking_strategy [assertion]
          early skip of rewriting module: openai.types.upload_complete_params [assertion]
          early skip of rewriting module: openai.types.embedding_create_params [assertion]
          early skip of rewriting module: openai.types.completion_create_params [assertion]
          early skip of rewriting module: openai.types.chat [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_message [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_audio [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_message_tool_call [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_token_logprob [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_role [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_chunk [assertion]
          early skip of rewriting module: openai.types.chat.completion_list_params [assertion]
          early skip of rewriting module: openai.types.chat.parsed_chat_completion [assertion]
          early skip of rewriting module: openai.types.chat.parsed_function_tool_call [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_deleted [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_modality [assertion]
          early skip of rewriting module: openai.types.chat.completion_create_params [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_tool_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_audio_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_tool_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_content_part_text_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_user_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_content_part_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_content_part_image_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_content_part_input_audio_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_system_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_function_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_assistant_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_message_tool_call_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_content_part_refusal_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_developer_message_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_stream_options_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_prediction_content_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_tool_choice_option_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_named_tool_choice_param [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_function_call_option_param [assertion]
          early skip of rewriting module: openai.types.chat.completion_update_params [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_store_message [assertion]
          early skip of rewriting module: openai.types.chat.chat_completion_reasoning_effort [assertion]
          early skip of rewriting module: openai.types.moderation_create_params [assertion]
          early skip of rewriting module: openai.types.moderation_multi_modal_input_param [assertion]
          early skip of rewriting module: openai.types.moderation_text_input_param [assertion]
          early skip of rewriting module: openai.types.moderation_image_url_input_param [assertion]
          early skip of rewriting module: openai.types.vector_store_list_params [assertion]
          early skip of rewriting module: openai.types.create_embedding_response [assertion]
          early skip of rewriting module: openai.types.moderation_create_response [assertion]
          early skip of rewriting module: openai.types.vector_store_create_params [assertion]
          early skip of rewriting module: openai.types.file_chunking_strategy_param [assertion]
          early skip of rewriting module: openai.types.auto_file_chunking_strategy_param [assertion]
          early skip of rewriting module: openai.types.static_file_chunking_strategy_object_param [assertion]
          early skip of rewriting module: openai.types.static_file_chunking_strategy_param [assertion]
          early skip of rewriting module: openai.types.vector_store_search_params [assertion]
          early skip of rewriting module: openai.types.vector_store_update_params [assertion]
          early skip of rewriting module: openai.types.vector_store_search_response [assertion]
          early skip of rewriting module: openai.types.websocket_connection_options [assertion]
          early skip of rewriting module: openai.types.image_create_variation_params [assertion]
          early skip of rewriting module: openai._client [assertion]
          early skip of rewriting module: openai._exceptions [assertion]
          early skip of rewriting module: openai._qs [assertion]
          early skip of rewriting module: openai._version [assertion]
          early skip of rewriting module: openai.resources [assertion]
          early skip of rewriting module: openai.resources.beta [assertion]
          early skip of rewriting module: openai.resources.beta.beta [assertion]
          early skip of rewriting module: openai.resources.beta.chat [assertion]
          early skip of rewriting module: openai.resources.beta.chat.chat [assertion]
          early skip of rewriting module: openai.resources.beta.chat.completions [assertion]
          early skip of rewriting module: openai._legacy_response [assertion]
          early skip of rewriting module: openai._streaming [assertion]
          early skip of rewriting module: openai._resource [assertion]
          early skip of rewriting module: openai._response [assertion]
          early skip of rewriting module: openai._base_client [assertion]
          early skip of rewriting module: distro [assertion]
          early skip of rewriting module: distro.distro [assertion]
          early skip of rewriting module: openai.lib [assertion]
          early skip of rewriting module: openai.lib._tools [assertion]
          early skip of rewriting module: openai.lib._pydantic [assertion]
          early skip of rewriting module: openai.types.responses [assertion]
          early skip of rewriting module: openai.types.responses.tool [assertion]
          early skip of rewriting module: openai.types.responses.computer_tool [assertion]
          early skip of rewriting module: openai.types.responses.function_tool [assertion]
          early skip of rewriting module: openai.types.responses.web_search_tool [assertion]
          early skip of rewriting module: openai.types.responses.file_search_tool [assertion]
          early skip of rewriting module: openai.types.responses.response [assertion]
          early skip of rewriting module: openai.types.responses.response_error [assertion]
          early skip of rewriting module: openai.types.responses.response_usage [assertion]
          early skip of rewriting module: openai.types.responses.response_status [assertion]
          early skip of rewriting module: openai.types.responses.tool_choice_types [assertion]
          early skip of rewriting module: openai.types.responses.tool_choice_options [assertion]
          early skip of rewriting module: openai.types.responses.response_output_item [assertion]
          early skip of rewriting module: openai.types.responses.response_output_message [assertion]
          early skip of rewriting module: openai.types.responses.response_output_text [assertion]
          early skip of rewriting module: openai.types.responses.response_output_refusal [assertion]
          early skip of rewriting module: openai.types.responses.response_reasoning_item [assertion]
          early skip of rewriting module: openai.types.responses.response_computer_tool_call [assertion]
          early skip of rewriting module: openai.types.responses.response_function_tool_call [assertion]
          early skip of rewriting module: openai.types.responses.response_function_web_search [assertion]
          early skip of rewriting module: openai.types.responses.response_file_search_tool_call [assertion]
          early skip of rewriting module: openai.types.responses.response_text_config [assertion]
          early skip of rewriting module: openai.types.responses.response_format_text_config [assertion]
          early skip of rewriting module: openai.types.responses.response_format_text_json_schema_config [assertion]
          early skip of rewriting module: openai.types.responses.tool_choice_function [assertion]
          early skip of rewriting module: openai.types.responses.tool_param [assertion]
          early skip of rewriting module: openai.types.responses.computer_tool_param [assertion]
          early skip of rewriting module: openai.types.responses.function_tool_param [assertion]
          early skip of rewriting module: openai.types.responses.web_search_tool_param [assertion]
          early skip of rewriting module: openai.types.responses.file_search_tool_param [assertion]
          early skip of rewriting module: openai.types.responses.response_item [assertion]
          early skip of rewriting module: openai.types.responses.response_input_message_item [assertion]
          early skip of rewriting module: openai.types.responses.response_input_message_content_list [assertion]
          early skip of rewriting module: openai.types.responses.response_input_content [assertion]
          early skip of rewriting module: openai.types.responses.response_input_file [assertion]
          early skip of rewriting module: openai.types.responses.response_input_text [assertion]
          early skip of rewriting module: openai.types.responses.response_input_image [assertion]
          early skip of rewriting module: openai.types.responses.response_function_tool_call_item [assertion]
          early skip of rewriting module: openai.types.responses.response_computer_tool_call_output_item [assertion]
          early skip of rewriting module: openai.types.responses.response_computer_tool_call_output_screenshot [assertion]
          early skip of rewriting module: openai.types.responses.response_function_tool_call_output_item [assertion]
          early skip of rewriting module: openai.types.responses.parsed_response [assertion]
          early skip of rewriting module: openai.types.responses.response_item_list [assertion]
          early skip of rewriting module: openai.types.responses.response_includable [assertion]
          early skip of rewriting module: openai.types.responses.response_error_event [assertion]
          early skip of rewriting module: openai.types.responses.response_input_param [assertion]
          early skip of rewriting module: openai.types.responses.easy_input_message_param [assertion]
          early skip of rewriting module: openai.types.responses.response_input_message_content_list_param [assertion]
          early skip of rewriting module: openai.types.responses.response_input_file_param [assertion]
          early skip of rewriting module: openai.types.responses.response_input_text_param [assertion]
          early skip of rewriting module: openai.types.responses.response_input_image_param [assertion]
          early skip of rewriting module: openai.types.responses.response_output_message_param [assertion]
          early skip of rewriting module: openai.types.responses.response_output_text_param [assertion]
          early skip of rewriting module: openai.types.responses.response_output_refusal_param [assertion]
          early skip of rewriting module: openai.types.responses.response_reasoning_item_param [assertion]
          early skip of rewriting module: openai.types.responses.response_computer_tool_call_param [assertion]
          early skip of rewriting module: openai.types.responses.response_function_tool_call_param [assertion]
          early skip of rewriting module: openai.types.responses.response_function_web_search_param [assertion]
          early skip of rewriting module: openai.types.responses.response_file_search_tool_call_param [assertion]
          early skip of rewriting module: openai.types.responses.response_computer_tool_call_output_screenshot_param [assertion]
          early skip of rewriting module: openai.types.responses.response_failed_event [assertion]
          early skip of rewriting module: openai.types.responses.response_stream_event [assertion]
          early skip of rewriting module: openai.types.responses.response_created_event [assertion]
          early skip of rewriting module: openai.types.responses.response_completed_event [assertion]
          early skip of rewriting module: openai.types.responses.response_text_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_audio_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_incomplete_event [assertion]
          early skip of rewriting module: openai.types.responses.response_text_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_audio_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_in_progress_event [assertion]
          early skip of rewriting module: openai.types.responses.response_refusal_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_refusal_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_output_item_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_content_part_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_output_item_added_event [assertion]
          early skip of rewriting module: openai.types.responses.response_content_part_added_event [assertion]
          early skip of rewriting module: openai.types.responses.response_audio_transcript_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_text_annotation_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_audio_transcript_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_web_search_call_completed_event [assertion]
          early skip of rewriting module: openai.types.responses.response_web_search_call_searching_event [assertion]
          early skip of rewriting module: openai.types.responses.response_file_search_call_completed_event [assertion]
          early skip of rewriting module: openai.types.responses.response_file_search_call_searching_event [assertion]
          early skip of rewriting module: openai.types.responses.response_web_search_call_in_progress_event [assertion]
          early skip of rewriting module: openai.types.responses.response_file_search_call_in_progress_event [assertion]
          early skip of rewriting module: openai.types.responses.response_function_call_arguments_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_function_call_arguments_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_call_code_done_event [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_call_completed_event [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_tool_call [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_call_code_delta_event [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_call_in_progress_event [assertion]
          early skip of rewriting module: openai.types.responses.response_code_interpreter_call_interpreting_event [assertion]
          early skip of rewriting module: openai.types.responses.input_item_list_params [assertion]
          early skip of rewriting module: openai.types.responses.response_create_params [assertion]
          early skip of rewriting module: openai.types.responses.tool_choice_types_param [assertion]
          early skip of rewriting module: openai.types.responses.response_text_config_param [assertion]
          early skip of rewriting module: openai.types.responses.response_format_text_config_param [assertion]
          early skip of rewriting module: openai.types.responses.response_format_text_json_schema_config_param [assertion]
          early skip of rewriting module: openai.types.responses.tool_choice_function_param [assertion]
          early skip of rewriting module: openai.types.responses.response_retrieve_params [assertion]
          early skip of rewriting module: openai.types.responses.response_input_item_param [assertion]
          early skip of rewriting module: openai.types.responses.response_input_content_param [assertion]
          early skip of rewriting module: openai.lib._parsing [assertion]
          early skip of rewriting module: openai.lib._parsing._completions [assertion]
          early skip of rewriting module: openai.lib.streaming [assertion]
          early skip of rewriting module: openai.lib.streaming._assistants [assertion]
          early skip of rewriting module: openai.types.beta [assertion]
          early skip of rewriting module: openai.types.beta.thread [assertion]
          early skip of rewriting module: openai.types.beta.assistant [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool [assertion]
          early skip of rewriting module: openai.types.beta.function_tool [assertion]
          early skip of rewriting module: openai.types.beta.file_search_tool [assertion]
          early skip of rewriting module: openai.types.beta.code_interpreter_tool [assertion]
          early skip of rewriting module: openai.types.beta.assistant_response_format_option [assertion]
          early skip of rewriting module: openai.types.beta.thread_deleted [assertion]
          early skip of rewriting module: openai.types.beta.assistant_deleted [assertion]
          early skip of rewriting module: openai.types.beta.function_tool_param [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_param [assertion]
          early skip of rewriting module: openai.types.beta.file_search_tool_param [assertion]
          early skip of rewriting module: openai.types.beta.code_interpreter_tool_param [assertion]
          early skip of rewriting module: openai.types.beta.thread_create_params [assertion]
          early skip of rewriting module: openai.types.beta.threads [assertion]
          early skip of rewriting module: openai.types.beta.threads.run [assertion]
          early skip of rewriting module: openai.types.beta.threads.run_status [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice_option [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice_function [assertion]
          early skip of rewriting module: openai.types.beta.threads.required_action_function_tool_call [assertion]
          early skip of rewriting module: openai.types.beta.threads.text [assertion]
          early skip of rewriting module: openai.types.beta.threads.annotation [assertion]
          early skip of rewriting module: openai.types.beta.threads.file_path_annotation [assertion]
          early skip of rewriting module: openai.types.beta.threads.file_citation_annotation [assertion]
          early skip of rewriting module: openai.types.beta.threads.message [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_content [assertion]
          early skip of rewriting module: openai.types.beta.threads.text_content_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.refusal_content_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url_content_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file_content_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file [assertion]
          early skip of rewriting module: openai.types.beta.threads.text_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.annotation_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.file_path_delta_annotation [assertion]
          early skip of rewriting module: openai.types.beta.threads.file_citation_delta_annotation [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_content_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.text_delta_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.refusal_delta_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url_delta_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file_delta_block [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_deleted [assertion]
          early skip of rewriting module: openai.types.beta.threads.run_list_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.run_create_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.run_step [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.tool_calls_step_details [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.tool_call [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.function_tool_call [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.file_search_tool_call [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.code_interpreter_tool_call [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.message_creation_step_details [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.run_step_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.tool_call_delta_object [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.tool_call_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.function_tool_call_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.file_search_tool_call_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.code_interpreter_tool_call_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.code_interpreter_logs [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.code_interpreter_output_image [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.run_step_delta_message_delta [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.run_step_include [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.step_list_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.run_step_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.threads.runs.step_retrieve_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_content_part_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.text_content_block_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_url_content_block_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.image_file_content_block_param [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice_option_param [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice_param [assertion]
          early skip of rewriting module: openai.types.beta.assistant_tool_choice_function_param [assertion]
          early skip of rewriting module: openai.types.beta.assistant_response_format_option_param [assertion]
          early skip of rewriting module: openai.types.beta.threads.run_update_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_list_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_create_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.message_update_params [assertion]
          early skip of rewriting module: openai.types.beta.threads.run_submit_tool_outputs_params [assertion]
          early skip of rewriting module: openai.types.beta.thread_update_params [assertion]
          early skip of rewriting module: openai.types.beta.assistant_list_params [assertion]
          early skip of rewriting module: openai.types.beta.assistant_stream_event [assertion]
          early skip of rewriting module: openai.types.beta.assistant_create_params [assertion]
          early skip of rewriting module: openai.types.beta.assistant_update_params [assertion]
          early skip of rewriting module: openai.types.beta.thread_create_and_run_params [assertion]
          early skip of rewriting module: openai.lib.streaming.chat [assertion]
          early skip of rewriting module: openai.lib.streaming.chat._types [assertion]
          early skip of rewriting module: openai.lib.streaming.chat._events [assertion]
          early skip of rewriting module: openai.lib.streaming.chat._completions [assertion]
          early skip of rewriting module: jiter [assertion]
          early skip of rewriting module: jiter.jiter [assertion]
          early skip of rewriting module: openai.lib.streaming._deltas [assertion]
          early skip of rewriting module: openai.resources.beta.assistants [assertion]
          early skip of rewriting module: openai.pagination [assertion]
          early skip of rewriting module: openai.resources.beta.threads [assertion]
          early skip of rewriting module: openai.resources.beta.threads.runs [assertion]
          early skip of rewriting module: openai.resources.beta.threads.runs.runs [assertion]
          early skip of rewriting module: openai.resources.beta.threads.runs.steps [assertion]
          early skip of rewriting module: openai.resources.beta.threads.threads [assertion]
          early skip of rewriting module: openai.resources.beta.threads.messages [assertion]
          early skip of rewriting module: openai.resources.beta.realtime [assertion]
          early skip of rewriting module: openai.resources.beta.realtime.realtime [assertion]
          early skip of rewriting module: openai.resources.beta.realtime.sessions [assertion]
          early skip of rewriting module: openai.types.beta.realtime [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session [assertion]
          early skip of rewriting module: openai.types.beta.realtime.error_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_content [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_response [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_response_usage [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_response_status [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_update_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_client_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_cancel_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_create_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_with_reference [assertion]
          early skip of rewriting module: openai.types.beta.realtime.transcription_session_update [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_create_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_delete_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_clear_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_append_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_commit_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_retrieve_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_truncate_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_server_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_created_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_updated_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_created_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_text_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.rate_limits_updated_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_audio_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_text_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_created_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_audio_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_created_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_deleted_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_output_item_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_cleared_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_content_part_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_output_item_added_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_truncated_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_content_part_added_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_committed_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.transcription_session_updated_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.transcription_session [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_audio_transcript_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_audio_transcript_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_speech_started_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_speech_stopped_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_function_call_arguments_done_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_function_call_arguments_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_input_audio_transcription_delta_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_input_audio_transcription_failed_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_input_audio_transcription_completed_event [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_create_params [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_content_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_connect_params [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_create_response [assertion]
          early skip of rewriting module: openai.types.beta.realtime.session_update_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.realtime_client_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_cancel_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.response_create_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_with_reference_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.transcription_session_update_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_create_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_delete_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_clear_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_append_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.input_audio_buffer_commit_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_retrieve_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.conversation_item_truncate_event_param [assertion]
          early skip of rewriting module: openai.types.beta.realtime.transcription_session_create_params [assertion]
          early skip of rewriting module: openai.resources.beta.realtime.transcription_sessions [assertion]
          early skip of rewriting module: openai.resources.chat [assertion]
          early skip of rewriting module: openai.resources.chat.chat [assertion]
          early skip of rewriting module: openai.resources.chat.completions [assertion]
          early skip of rewriting module: openai.resources.chat.completions.messages [assertion]
          early skip of rewriting module: openai.types.chat.completions [assertion]
          early skip of rewriting module: openai.types.chat.completions.message_list_params [assertion]
          early skip of rewriting module: openai.resources.chat.completions.completions [assertion]
          early skip of rewriting module: openai.resources.audio [assertion]
          early skip of rewriting module: openai.resources.audio.audio [assertion]
          early skip of rewriting module: openai.resources.audio.speech [assertion]
          early skip of rewriting module: openai.types.audio [assertion]
          early skip of rewriting module: openai.types.audio.translation [assertion]
          early skip of rewriting module: openai.types.audio.speech_model [assertion]
          early skip of rewriting module: openai.types.audio.transcription [assertion]
          early skip of rewriting module: openai.types.audio.transcription_word [assertion]
          early skip of rewriting module: openai.types.audio.translation_verbose [assertion]
          early skip of rewriting module: openai.types.audio.transcription_segment [assertion]
          early skip of rewriting module: openai.types.audio.speech_create_params [assertion]
          early skip of rewriting module: openai.types.audio.transcription_include [assertion]
          early skip of rewriting module: openai.types.audio.transcription_verbose [assertion]
          early skip of rewriting module: openai.types.audio.translation_create_params [assertion]
          early skip of rewriting module: openai.types.audio.transcription_stream_event [assertion]
          early skip of rewriting module: openai.types.audio.transcription_text_done_event [assertion]
          early skip of rewriting module: openai.types.audio.transcription_text_delta_event [assertion]
          early skip of rewriting module: openai.types.audio.transcription_create_params [assertion]
          early skip of rewriting module: openai.types.audio.translation_create_response [assertion]
          early skip of rewriting module: openai.types.audio.transcription_create_response [assertion]
          early skip of rewriting module: openai.resources.audio.translations [assertion]
          early skip of rewriting module: openai.resources.audio.transcriptions [assertion]
          early skip of rewriting module: openai.resources.evals [assertion]
          early skip of rewriting module: openai.resources.evals.runs [assertion]
          early skip of rewriting module: openai.resources.evals.runs.runs [assertion]
          early skip of rewriting module: openai.resources.evals.runs.output_items [assertion]
          early skip of rewriting module: openai.types.evals [assertion]
          early skip of rewriting module: openai.types.evals.eval_api_error [assertion]
          early skip of rewriting module: openai.types.evals.run_list_params [assertion]
          early skip of rewriting module: openai.types.evals.run_create_params [assertion]
          early skip of rewriting module: openai.types.evals.create_eval_jsonl_run_data_source_param [assertion]
          early skip of rewriting module: openai.types.evals.create_eval_completions_run_data_source_param [assertion]
          early skip of rewriting module: openai.types.evals.run_list_response [assertion]
          early skip of rewriting module: openai.types.evals.create_eval_jsonl_run_data_source [assertion]
          early skip of rewriting module: openai.types.evals.create_eval_completions_run_data_source [assertion]
          early skip of rewriting module: openai.types.evals.run_cancel_response [assertion]
          early skip of rewriting module: openai.types.evals.run_create_response [assertion]
          early skip of rewriting module: openai.types.evals.run_delete_response [assertion]
          early skip of rewriting module: openai.types.evals.run_retrieve_response [assertion]
          early skip of rewriting module: openai.types.evals.runs [assertion]
          early skip of rewriting module: openai.types.evals.runs.output_item_list_params [assertion]
          early skip of rewriting module: openai.types.evals.runs.output_item_list_response [assertion]
          early skip of rewriting module: openai.types.evals.runs.output_item_retrieve_response [assertion]
          early skip of rewriting module: openai.resources.evals.evals [assertion]
          early skip of rewriting module: openai.resources.files [assertion]
          early skip of rewriting module: openai.resources.images [assertion]
          early skip of rewriting module: openai.resources.models [assertion]
          early skip of rewriting module: openai.resources.batches [assertion]
          early skip of rewriting module: openai.resources.uploads [assertion]
          early skip of rewriting module: openai.resources.uploads.parts [assertion]
          early skip of rewriting module: openai.types.uploads [assertion]
          early skip of rewriting module: openai.types.uploads.upload_part [assertion]
          early skip of rewriting module: openai.types.uploads.part_create_params [assertion]
          early skip of rewriting module: openai.resources.uploads.uploads [assertion]
          early skip of rewriting module: openai.resources.responses [assertion]
          early skip of rewriting module: openai.resources.responses.responses [assertion]
          early skip of rewriting module: openai.resources.responses.input_items [assertion]
          early skip of rewriting module: openai.lib._parsing._responses [assertion]
          early skip of rewriting module: openai.lib.streaming.responses [assertion]
          early skip of rewriting module: openai.lib.streaming.responses._events [assertion]
          early skip of rewriting module: openai.lib.streaming.responses._responses [assertion]
          early skip of rewriting module: openai.lib.streaming.responses._types [assertion]
          early skip of rewriting module: openai.resources.embeddings [assertion]
          early skip of rewriting module: openai._extras [assertion]
          early skip of rewriting module: openai._extras.numpy_proxy [assertion]
          early skip of rewriting module: openai._extras._common [assertion]
          early skip of rewriting module: openai._extras.pandas_proxy [assertion]
          early skip of rewriting module: openai._extras.sounddevice_proxy [assertion]
          early skip of rewriting module: openai.resources.completions [assertion]
          early skip of rewriting module: openai.resources.fine_tuning [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.jobs [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.jobs.jobs [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.jobs.checkpoints [assertion]
          early skip of rewriting module: openai.types.fine_tuning [assertion]
          early skip of rewriting module: openai.types.fine_tuning.fine_tuning_job [assertion]
          early skip of rewriting module: openai.types.fine_tuning.fine_tuning_job_wandb_integration_object [assertion]
          early skip of rewriting module: openai.types.fine_tuning.fine_tuning_job_wandb_integration [assertion]
          early skip of rewriting module: openai.types.fine_tuning.job_list_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.job_create_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.fine_tuning_job_event [assertion]
          early skip of rewriting module: openai.types.fine_tuning.job_list_events_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.fine_tuning_job_integration [assertion]
          early skip of rewriting module: openai.types.fine_tuning.jobs [assertion]
          early skip of rewriting module: openai.types.fine_tuning.jobs.checkpoint_list_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.jobs.fine_tuning_job_checkpoint [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.checkpoints [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.checkpoints.checkpoints [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.checkpoints.permissions [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints.permission_create_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints.permission_create_response [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints.permission_delete_response [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints.permission_retrieve_params [assertion]
          early skip of rewriting module: openai.types.fine_tuning.checkpoints.permission_retrieve_response [assertion]
          early skip of rewriting module: openai.resources.fine_tuning.fine_tuning [assertion]
          early skip of rewriting module: openai.resources.moderations [assertion]
          early skip of rewriting module: openai.resources.vector_stores [assertion]
          early skip of rewriting module: openai.resources.vector_stores.files [assertion]
          early skip of rewriting module: openai.types.vector_stores [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_list_params [assertion]
          early skip of rewriting module: openai.types.vector_stores.vector_store_file [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_create_params [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_update_params [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_content_response [assertion]
          early skip of rewriting module: openai.types.vector_stores.vector_store_file_batch [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_batch_create_params [assertion]
          early skip of rewriting module: openai.types.vector_stores.vector_store_file_deleted [assertion]
          early skip of rewriting module: openai.types.vector_stores.file_batch_list_files_params [assertion]
          early skip of rewriting module: openai.resources.vector_stores.file_batches [assertion]
          early skip of rewriting module: concurrent.futures.thread [assertion]
          early skip of rewriting module: queue [assertion]
          early skip of rewriting module: _queue [assertion]
          early skip of rewriting module: openai.resources.vector_stores.vector_stores [assertion]
          early skip of rewriting module: openai.lib.azure [assertion]
          early skip of rewriting module: openai.version [assertion]
          early skip of rewriting module: openai.lib._old_api [assertion]
          early skip of rewriting module: openai._module_client [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.core_helpers [assertion]
          early skip of rewriting module: litellm.types.llms [assertion]
          early skip of rewriting module: litellm.types.llms.openai [assertion]
          early skip of rewriting module: litellm.types.guardrails [assertion]
          early skip of rewriting module: litellm.types.rerank [assertion]
          early skip of rewriting module: litellm.types.llms.custom_http [assertion]
          early skip of rewriting module: litellm._version [assertion]
          early skip of rewriting module: importlib_metadata [assertion]
          early skip of rewriting module: zipp [assertion]
          early skip of rewriting module: zipp.compat [assertion]
          early skip of rewriting module: zipp.compat.py310 [assertion]
          early skip of rewriting module: zipp.glob [assertion]
          early skip of rewriting module: importlib_metadata._meta [assertion]
          early skip of rewriting module: importlib_metadata.compat [assertion]
          early skip of rewriting module: importlib_metadata.compat.py39 [assertion]
          early skip of rewriting module: importlib_metadata.compat.py311 [assertion]
          early skip of rewriting module: importlib_metadata._collections [assertion]
          early skip of rewriting module: importlib_metadata._compat [assertion]
          early skip of rewriting module: importlib_metadata._functools [assertion]
          early skip of rewriting module: importlib_metadata._itertools [assertion]
          early skip of rewriting module: importlib_metadata._adapters [assertion]
          early skip of rewriting module: importlib_metadata._text [assertion]
          early skip of rewriting module: litellm.caching [assertion]
          early skip of rewriting module: litellm.caching.caching [assertion]
          early skip of rewriting module: litellm.constants [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.model_param_helper [assertion]
          early skip of rewriting module: litellm.types.caching [assertion]
          early skip of rewriting module: litellm.caching.base_cache [assertion]
          early skip of rewriting module: litellm.caching.disk_cache [assertion]
          early skip of rewriting module: litellm.caching.dual_cache [assertion]
          early skip of rewriting module: litellm.caching.in_memory_cache [assertion]
          early skip of rewriting module: litellm.caching.redis_cache [assertion]
          early skip of rewriting module: litellm.types.services [assertion]
          early skip of rewriting module: litellm.caching.qdrant_semantic_cache [assertion]
          early skip of rewriting module: litellm.caching.redis_cluster_cache [assertion]
          early skip of rewriting module: litellm.caching.redis_semantic_cache [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.prompt_templates [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.prompt_templates.common_utils [assertion]
          early skip of rewriting module: litellm.caching.s3_cache [assertion]
          early skip of rewriting module: litellm.caching.llm_caching_handler [assertion]
          early skip of rewriting module: litellm.types.llms.bedrock [assertion]
          early skip of rewriting module: litellm.proxy [assertion]
          early skip of rewriting module: litellm.proxy._types [assertion]
          early skip of rewriting module: litellm.types.integrations [assertion]
          early skip of rewriting module: litellm.types.integrations.slack_alerting [assertion]
          early skip of rewriting module: litellm.types.router [assertion]
          early skip of rewriting module: litellm.exceptions [assertion]
          early skip of rewriting module: litellm.types.completion [assertion]
          early skip of rewriting module: litellm.types.embedding [assertion]
          early skip of rewriting module: litellm.types.llms.vertex_ai [assertion]
          early skip of rewriting module: litellm.proxy.types_utils [assertion]
          early skip of rewriting module: litellm.proxy.types_utils.utils [assertion]
          early skip of rewriting module: litellm.integrations [assertion]
          early skip of rewriting module: litellm.integrations.custom_logger [assertion]
          early skip of rewriting module: litellm.types.integrations.argilla [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.logging_callback_manager [assertion]
          early skip of rewriting module: litellm.integrations.additional_logging_utils [assertion]
          early skip of rewriting module: litellm.types.integrations.base_health_check [assertion]
          early skip of rewriting module: dotenv [assertion]
          early skip of rewriting module: dotenv.main [assertion]
          early skip of rewriting module: dotenv.parser [assertion]
          early skip of rewriting module: dotenv.variables [assertion]
          early skip of rewriting module: httpcore [assertion]
          early skip of rewriting module: httpcore._api [assertion]
          early skip of rewriting module: httpcore._models [assertion]
          early skip of rewriting module: httpcore._sync [assertion]
          early skip of rewriting module: httpcore._sync.connection [assertion]
          early skip of rewriting module: httpcore._backends [assertion]
          early skip of rewriting module: httpcore._backends.sync [assertion]
          early skip of rewriting module: httpcore._exceptions [assertion]
          early skip of rewriting module: httpcore._utils [assertion]
          early skip of rewriting module: httpcore._backends.base [assertion]
          early skip of rewriting module: httpcore._ssl [assertion]
          early skip of rewriting module: certifi [assertion]
          early skip of rewriting module: certifi.core [assertion]
          early skip of rewriting module: importlib.resources [assertion]
          early skip of rewriting module: httpcore._synchronization [assertion]
          early skip of rewriting module: trio [assertion]
          early skip of rewriting module: trio._core [assertion]
          early skip of rewriting module: trio._core._entry_queue [assertion]
          early skip of rewriting module: attrs [assertion]
          early skip of rewriting module: attrs.converters [assertion]
          early skip of rewriting module: attrs.exceptions [assertion]
          early skip of rewriting module: attrs.filters [assertion]
          early skip of rewriting module: attrs.setters [assertion]
          early skip of rewriting module: attrs.validators [assertion]
          early skip of rewriting module: trio._util [assertion]
          early skip of rewriting module: trio._core._wakeup_socketpair [assertion]
          early skip of rewriting module: trio._core._exceptions [assertion]
          early skip of rewriting module: trio._core._ki [assertion]
          early skip of rewriting module: trio._core._local [assertion]
          early skip of rewriting module: trio._core._run [assertion]
          early skip of rewriting module: outcome [assertion]
          early skip of rewriting module: outcome._impl [assertion]
          early skip of rewriting module: outcome._util [assertion]
          early skip of rewriting module: outcome._version [assertion]
          early skip of rewriting module: sortedcontainers [assertion]
          early skip of rewriting module: sortedcontainers.sortedlist [assertion]
          early skip of rewriting module: sortedcontainers.sortedset [assertion]
          early skip of rewriting module: sortedcontainers.sorteddict [assertion]
          early skip of rewriting module: trio._abc [assertion]
          early skip of rewriting module: trio._deprecate [assertion]
          early skip of rewriting module: trio._core._asyncgens [assertion]
          early skip of rewriting module: trio._core._concat_tb [assertion]
          early skip of rewriting module: tputil [assertion]
          early skip of rewriting module: _ctypes [assertion]
          early skip of rewriting module: ctypes [assertion]
          early skip of rewriting module: ctypes._endian [assertion]
          early skip of rewriting module: trio._core._instrumentation [assertion]
          early skip of rewriting module: trio._core._thread_cache [assertion]
          early skip of rewriting module: ctypes.util [assertion]
          early skip of rewriting module: trio._core._traps [assertion]
          early skip of rewriting module: trio._core._generated_io_epoll [assertion]
          early skip of rewriting module: trio._core._io_epoll [assertion]
          early skip of rewriting module: trio._core._io_common [assertion]
          early skip of rewriting module: trio._core._generated_instrumentation [assertion]
          early skip of rewriting module: trio._core._generated_run [assertion]
          early skip of rewriting module: trio._core._mock_clock [assertion]
          early skip of rewriting module: trio._core._parking_lot [assertion]
          early skip of rewriting module: trio._core._unbounded_queue [assertion]
          early skip of rewriting module: trio.abc [assertion]
          early skip of rewriting module: trio.from_thread [assertion]
          early skip of rewriting module: trio._threads [assertion]
          early skip of rewriting module: trio._sync [assertion]
          early skip of rewriting module: trio.lowlevel [assertion]
          early skip of rewriting module: trio._subprocess [assertion]
          early skip of rewriting module: trio._highlevel_generic [assertion]
          early skip of rewriting module: trio._subprocess_platform [assertion]
          early skip of rewriting module: trio._subprocess_platform.waitid [assertion]
          early skip of rewriting module: trio._unix_pipes [assertion]
          early skip of rewriting module: trio.socket [assertion]
          early skip of rewriting module: trio._socket [assertion]
          early skip of rewriting module: trio.to_thread [assertion]
          early skip of rewriting module: trio._channel [assertion]
          early skip of rewriting module: trio._dtls [assertion]
          early skip of rewriting module: hmac [assertion]
          early skip of rewriting module: trio._file_io [assertion]
          early skip of rewriting module: trio._highlevel_open_tcp_listeners [assertion]
          early skip of rewriting module: trio._highlevel_open_tcp_stream [assertion]
          early skip of rewriting module: trio._highlevel_open_unix_stream [assertion]
          early skip of rewriting module: trio._highlevel_serve_listeners [assertion]
          early skip of rewriting module: trio._highlevel_socket [assertion]
          early skip of rewriting module: trio._highlevel_ssl_helpers [assertion]
          early skip of rewriting module: trio._path [assertion]
          early skip of rewriting module: trio._signals [assertion]
          early skip of rewriting module: trio._ssl [assertion]
          early skip of rewriting module: trio._timeouts [assertion]
          early skip of rewriting module: trio._version [assertion]
          early skip of rewriting module: httpcore._trace [assertion]
          early skip of rewriting module: httpcore._sync.http11 [assertion]
          early skip of rewriting module: h11 [assertion]
          early skip of rewriting module: h11._connection [assertion]
          early skip of rewriting module: h11._events [assertion]
          early skip of rewriting module: h11._abnf [assertion]
          early skip of rewriting module: h11._headers [assertion]
          early skip of rewriting module: h11._util [assertion]
          early skip of rewriting module: h11._readers [assertion]
          early skip of rewriting module: h11._receivebuffer [assertion]
          early skip of rewriting module: h11._state [assertion]
          early skip of rewriting module: h11._writers [assertion]
          early skip of rewriting module: h11._version [assertion]
          early skip of rewriting module: httpcore._sync.interfaces [assertion]
          early skip of rewriting module: httpcore._sync.connection_pool [assertion]
          early skip of rewriting module: httpcore._sync.http_proxy [assertion]
          early skip of rewriting module: httpcore._sync.http2 [assertion]
          early skip of rewriting module: h2 [assertion]
          early skip of rewriting module: httpcore._sync.socks_proxy [assertion]
          early skip of rewriting module: socksio [assertion]
          early skip of rewriting module: httpcore._async [assertion]
          early skip of rewriting module: httpcore._async.connection [assertion]
          early skip of rewriting module: httpcore._backends.auto [assertion]
          early skip of rewriting module: httpcore._async.http11 [assertion]
          early skip of rewriting module: httpcore._async.interfaces [assertion]
          early skip of rewriting module: httpcore._async.connection_pool [assertion]
          early skip of rewriting module: httpcore._async.http_proxy [assertion]
          early skip of rewriting module: httpcore._async.http2 [assertion]
          early skip of rewriting module: h2 [assertion]
          early skip of rewriting module: httpcore._async.socks_proxy [assertion]
          early skip of rewriting module: socksio [assertion]
          early skip of rewriting module: httpcore._backends.mock [assertion]
          early skip of rewriting module: httpcore._backends.anyio [assertion]
          early skip of rewriting module: httpcore._backends.trio [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.get_model_cost_map [assertion]
          early skip of rewriting module: encodings.idna [assertion]
          early skip of rewriting module: stringprep [assertion]
          early skip of rewriting module: litellm.timeout [assertion]
          early skip of rewriting module: litellm.cost_calculator [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_cost_calc [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_cost_calc.tool_call_cost_tracking [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_cost_calc.utils [assertion]
          early skip of rewriting module: litellm.utils [assertion]
          early skip of rewriting module: tiktoken [assertion]
          early skip of rewriting module: tiktoken.core [assertion]
          early skip of rewriting module: regex [assertion]
          early skip of rewriting module: regex.regex [assertion]
          early skip of rewriting module: regex._regex_core [assertion]
          early skip of rewriting module: regex._regex [assertion]
          early skip of rewriting module: regex.DEFAULT_VERSION [assertion]
          early skip of rewriting module: tiktoken._tiktoken [assertion]
          early skip of rewriting module: tiktoken.model [assertion]
          early skip of rewriting module: tiktoken.registry [assertion]
          early skip of rewriting module: pkgutil [assertion]
          early skip of rewriting module: tiktoken_ext [assertion]
          early skip of rewriting module: tokenizers [assertion]
          early skip of rewriting module: tokenizers.tokenizers [assertion]
          early skip of rewriting module: tokenizers.implementations [assertion]
          early skip of rewriting module: tokenizers.implementations.base_tokenizer [assertion]
          early skip of rewriting module: tokenizers.decoders [assertion]
          early skip of rewriting module: tokenizers.models [assertion]
          early skip of rewriting module: tokenizers.normalizers [assertion]
          early skip of rewriting module: tokenizers.pre_tokenizers [assertion]
          early skip of rewriting module: tokenizers.processors [assertion]
          early skip of rewriting module: tokenizers.implementations.bert_wordpiece [assertion]
          early skip of rewriting module: tokenizers.implementations.byte_level_bpe [assertion]
          early skip of rewriting module: tokenizers.implementations.char_level_bpe [assertion]
          early skip of rewriting module: tokenizers.implementations.sentencepiece_bpe [assertion]
          early skip of rewriting module: tokenizers.implementations.sentencepiece_unigram [assertion]
          early skip of rewriting module: litellm._service_logger [assertion]
          early skip of rewriting module: litellm.integrations.datadog [assertion]
          early skip of rewriting module: litellm.integrations.datadog.datadog [assertion]
          early skip of rewriting module: litellm.integrations.custom_batch_logger [assertion]
          early skip of rewriting module: litellm.types.integrations.datadog [assertion]
          early skip of rewriting module: litellm.integrations.opentelemetry [assertion]
          early skip of rewriting module: litellm.integrations.prometheus_services [assertion]
          early skip of rewriting module: litellm.types.integrations.prometheus [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.audio_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.audio_utils.utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.json_validation_rule [assertion]
          early skip of rewriting module: litellm.llms.gemini [assertion]
          early skip of rewriting module: litellm.caching._internal_lru_cache [assertion]
          early skip of rewriting module: litellm.caching.caching_handler [assertion]
          early skip of rewriting module: litellm.integrations.custom_guardrail [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.credential_accessor [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.default_encoding [assertion]
          early skip of rewriting module: pkg_resources [assertion]
          early skip of rewriting module: plistlib [assertion]
          early skip of rewriting module: xml.parsers [assertion]
          early skip of rewriting module: xml.parsers.expat [assertion]
          early skip of rewriting module: pkg_resources.extern [assertion]
          early skip of rewriting module: pkg_resources.extern.six [assertion]
          early skip of rewriting module: pkg_resources._vendor [assertion]
          early skip of rewriting module: pkg_resources._vendor.six [assertion]
          early skip of rewriting module: pkg_resources.extern.six.moves [assertion]
          early skip of rewriting module: pkg_resources._vendor.six [assertion]
          early skip of rewriting module: pkg_resources._vendor.six.moves [assertion]
          early skip of rewriting module: pkg_resources._vendor.six.moves [assertion]
          early skip of rewriting module: pkg_resources._vendor.six.moves.urllib [assertion]
          early skip of rewriting module: pkg_resources.py31compat [assertion]
          early skip of rewriting module: pkg_resources.extern.appdirs [assertion]
          early skip of rewriting module: pkg_resources._vendor.appdirs [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging [assertion]
          early skip of rewriting module: pkg_resources._vendor.packaging [assertion]
          early skip of rewriting module: pkg_resources._vendor.packaging.__about__ [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging.version [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging._structures [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging.specifiers [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging._compat [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging.requirements [assertion]
          early skip of rewriting module: pkg_resources.extern.pyparsing [assertion]
          early skip of rewriting module: pkg_resources._vendor.pyparsing [assertion]
          early skip of rewriting module: pkg_resources.extern.six.moves.urllib [assertion]
          early skip of rewriting module: pkg_resources.extern.packaging.markers [assertion]
          early skip of rewriting module: pkg_resources.py2_warn [assertion]
          early skip of rewriting module: tiktoken_ext.openai_public [assertion]
          early skip of rewriting module: tiktoken.load [assertion]
          early skip of rewriting module: requests [assertion]
          early skip of rewriting module: urllib3 [assertion]
          early skip of rewriting module: urllib3.exceptions [assertion]
          early skip of rewriting module: urllib3._base_connection [assertion]
          early skip of rewriting module: urllib3.util [assertion]
          early skip of rewriting module: urllib3.util.connection [assertion]
          early skip of rewriting module: urllib3.util.timeout [assertion]
          early skip of rewriting module: urllib3.util.request [assertion]
          early skip of rewriting module: urllib3.util.util [assertion]
          early skip of rewriting module: brotlicffi [assertion]
          early skip of rewriting module: zstandard [assertion]
          early skip of rewriting module: urllib3.util.response [assertion]
          early skip of rewriting module: urllib3.util.retry [assertion]
          early skip of rewriting module: urllib3.util.ssl_ [assertion]
          early skip of rewriting module: urllib3.util.url [assertion]
          early skip of rewriting module: urllib3.util.ssltransport [assertion]
          early skip of rewriting module: urllib3.util.wait [assertion]
          early skip of rewriting module: urllib3._collections [assertion]
          early skip of rewriting module: urllib3._version [assertion]
          early skip of rewriting module: urllib3.connectionpool [assertion]
          early skip of rewriting module: urllib3._request_methods [assertion]
          early skip of rewriting module: urllib3.filepost [assertion]
          early skip of rewriting module: urllib3.fields [assertion]
          early skip of rewriting module: urllib3.response [assertion]
          early skip of rewriting module: brotlicffi [assertion]
          early skip of rewriting module: zstandard [assertion]
          early skip of rewriting module: urllib3.connection [assertion]
          early skip of rewriting module: urllib3.http2 [assertion]
          early skip of rewriting module: urllib3.http2.probe [assertion]
          early skip of rewriting module: urllib3.util.ssl_match_hostname [assertion]
          early skip of rewriting module: urllib3.util.proxy [assertion]
          early skip of rewriting module: urllib3.poolmanager [assertion]
          early skip of rewriting module: requests.exceptions [assertion]
          early skip of rewriting module: requests.compat [assertion]
          early skip of rewriting module: chardet [assertion]
          early skip of rewriting module: chardet.compat [assertion]
          early skip of rewriting module: chardet.universaldetector [assertion]
          early skip of rewriting module: chardet.charsetgroupprober [assertion]
          early skip of rewriting module: chardet.enums [assertion]
          early skip of rewriting module: chardet.charsetprober [assertion]
          early skip of rewriting module: chardet.escprober [assertion]
          early skip of rewriting module: chardet.codingstatemachine [assertion]
          early skip of rewriting module: chardet.escsm [assertion]
          early skip of rewriting module: chardet.latin1prober [assertion]
          early skip of rewriting module: chardet.mbcsgroupprober [assertion]
          early skip of rewriting module: chardet.utf8prober [assertion]
          early skip of rewriting module: chardet.mbcssm [assertion]
          early skip of rewriting module: chardet.sjisprober [assertion]
          early skip of rewriting module: chardet.mbcharsetprober [assertion]
          early skip of rewriting module: chardet.chardistribution [assertion]
          early skip of rewriting module: chardet.euctwfreq [assertion]
          early skip of rewriting module: chardet.euckrfreq [assertion]
          early skip of rewriting module: chardet.gb2312freq [assertion]
          early skip of rewriting module: chardet.big5freq [assertion]
          early skip of rewriting module: chardet.jisfreq [assertion]
          early skip of rewriting module: chardet.jpcntx [assertion]
          early skip of rewriting module: chardet.eucjpprober [assertion]
          early skip of rewriting module: chardet.gb2312prober [assertion]
          early skip of rewriting module: chardet.euckrprober [assertion]
          early skip of rewriting module: chardet.cp949prober [assertion]
          early skip of rewriting module: chardet.big5prober [assertion]
          early skip of rewriting module: chardet.euctwprober [assertion]
          early skip of rewriting module: chardet.sbcsgroupprober [assertion]
          early skip of rewriting module: chardet.sbcharsetprober [assertion]
          early skip of rewriting module: chardet.langcyrillicmodel [assertion]
          early skip of rewriting module: chardet.langgreekmodel [assertion]
          early skip of rewriting module: chardet.langbulgarianmodel [assertion]
          early skip of rewriting module: chardet.langthaimodel [assertion]
          early skip of rewriting module: chardet.langhebrewmodel [assertion]
          early skip of rewriting module: chardet.hebrewprober [assertion]
          early skip of rewriting module: chardet.langturkishmodel [assertion]
          early skip of rewriting module: chardet.version [assertion]
          early skip of rewriting module: simplejson [assertion]
          early skip of rewriting module: simplejson.errors [assertion]
          early skip of rewriting module: simplejson.raw_json [assertion]
          early skip of rewriting module: simplejson.decoder [assertion]
          early skip of rewriting module: simplejson.compat [assertion]
          early skip of rewriting module: simplejson.scanner [assertion]
          early skip of rewriting module: simplejson._speedups [assertion]
          early skip of rewriting module: simplejson.encoder [assertion]
          early skip of rewriting module: requests.packages [assertion]
          early skip of rewriting module: requests.utils [assertion]
          early skip of rewriting module: requests.certs [assertion]
          early skip of rewriting module: requests.__version__ [assertion]
          early skip of rewriting module: requests._internal_utils [assertion]
          early skip of rewriting module: requests.cookies [assertion]
          early skip of rewriting module: requests.structures [assertion]
          early skip of rewriting module: requests.api [assertion]
          early skip of rewriting module: requests.sessions [assertion]
          early skip of rewriting module: requests.adapters [assertion]
          early skip of rewriting module: requests.auth [assertion]
          early skip of rewriting module: requests.models [assertion]
          early skip of rewriting module: requests.hooks [assertion]
          early skip of rewriting module: requests.status_codes [assertion]
          early skip of rewriting module: urllib3.contrib [assertion]
          early skip of rewriting module: urllib3.contrib.socks [assertion]
          early skip of rewriting module: socks [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.exception_mapping_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.get_litellm_params [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.get_llm_provider_logic [assertion]
          early skip of rewriting module: litellm.secret_managers [assertion]
          early skip of rewriting module: litellm.secret_managers.main [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.get_supported_openai_params [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_request_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils.convert_dict_to_response [assertion]
          early skip of rewriting module: litellm.types.llms.databricks [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils.get_headers [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils.get_api_base [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils.get_formatted_prompt [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.llm_response_utils.response_metadata [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.redact_messages [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.rules [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.streaming_handler [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.thread_pool_executor [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.token_counter [assertion]
          early skip of rewriting module: litellm.llms.bedrock [assertion]
          early skip of rewriting module: litellm.llms.bedrock.common_utils [assertion]
          early skip of rewriting module: litellm.llms.base_llm [assertion]
          early skip of rewriting module: litellm.llms.base_llm.base_utils [assertion]
          early skip of rewriting module: litellm.llms.base_llm.chat [assertion]
          early skip of rewriting module: litellm.llms.base_llm.chat.transformation [assertion]
          early skip of rewriting module: litellm.router_utils [assertion]
          early skip of rewriting module: litellm.router_utils.get_retry_from_policy [assertion]
          early skip of rewriting module: litellm.types.llms.anthropic [assertion]
          early skip of rewriting module: pydantic.deprecated [assertion]
          early skip of rewriting module: pydantic.deprecated.class_validators [assertion]
          early skip of rewriting module: pydantic._internal._decorators_v1 [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.tokenizers [assertion]
          early skip of rewriting module: litellm.llms.base_llm.anthropic_messages [assertion]
          early skip of rewriting module: litellm.llms.base_llm.anthropic_messages.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.audio_transcription [assertion]
          early skip of rewriting module: litellm.llms.base_llm.audio_transcription.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.completion [assertion]
          early skip of rewriting module: litellm.llms.base_llm.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.embedding [assertion]
          early skip of rewriting module: litellm.llms.base_llm.embedding.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.files [assertion]
          early skip of rewriting module: litellm.llms.base_llm.files.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.image_variations [assertion]
          early skip of rewriting module: litellm.llms.base_llm.image_variations.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.rerank [assertion]
          early skip of rewriting module: litellm.llms.base_llm.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.responses [assertion]
          early skip of rewriting module: litellm.llms.base_llm.responses.transformation [assertion]
          early skip of rewriting module: litellm.llms.anthropic [assertion]
          early skip of rewriting module: litellm.llms.anthropic.cost_calculation [assertion]
          early skip of rewriting module: litellm.llms.azure [assertion]
          early skip of rewriting module: litellm.llms.azure.cost_calculation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.databricks [assertion]
          early skip of rewriting module: litellm.llms.databricks.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.deepseek [assertion]
          early skip of rewriting module: litellm.llms.deepseek.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.gemini.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.openai [assertion]
          early skip of rewriting module: litellm.llms.openai.cost_calculation [assertion]
          early skip of rewriting module: litellm.llms.together_ai [assertion]
          early skip of rewriting module: litellm.llms.together_ai.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.cost_calculator [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.image_generation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.image_generation.cost_calculator [assertion]
          early skip of rewriting module: litellm.responses [assertion]
          early skip of rewriting module: litellm.responses.utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.litellm_logging [assertion]
          early skip of rewriting module: litellm.batches [assertion]
          early skip of rewriting module: litellm.batches.batch_utils [assertion]
          early skip of rewriting module: litellm.integrations.arize [assertion]
          early skip of rewriting module: litellm.integrations.arize.arize [assertion]
          early skip of rewriting module: litellm.integrations.arize._utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.safe_json_dumps [assertion]
          early skip of rewriting module: litellm.types.integrations.arize [assertion]
          early skip of rewriting module: litellm.integrations.mlflow [assertion]
          early skip of rewriting module: litellm.integrations.pagerduty [assertion]
          early skip of rewriting module: litellm.integrations.pagerduty.pagerduty [assertion]
          early skip of rewriting module: litellm.integrations.SlackAlerting [assertion]
          early skip of rewriting module: litellm.integrations.SlackAlerting.slack_alerting [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.duration_parser [assertion]
          early skip of rewriting module: litellm.integrations.email_templates [assertion]
          early skip of rewriting module: litellm.integrations.email_templates.templates [assertion]
          early skip of rewriting module: litellm.integrations.SlackAlerting.batching_handler [assertion]
          early skip of rewriting module: litellm.integrations.SlackAlerting.utils [assertion]
          early skip of rewriting module: litellm.types.integrations.pagerduty [assertion]
          early skip of rewriting module: litellm.integrations.argilla [assertion]
          early skip of rewriting module: litellm.integrations.arize.arize_phoenix [assertion]
          early skip of rewriting module: litellm.types.integrations.arize_phoenix [assertion]
          early skip of rewriting module: litellm.integrations.athina [assertion]
          early skip of rewriting module: litellm.integrations.azure_storage [assertion]
          early skip of rewriting module: litellm.integrations.azure_storage.azure_storage [assertion]
          early skip of rewriting module: litellm.llms.azure.common_utils [assertion]
          early skip of rewriting module: litellm.llms.openai.common_utils [assertion]
          early skip of rewriting module: litellm.secret_managers.get_azure_ad_token_provider [assertion]
          early skip of rewriting module: litellm.integrations.braintrust_logging [assertion]
          early skip of rewriting module: litellm.integrations.custom_prompt_management [assertion]
          early skip of rewriting module: litellm.integrations.prompt_management_base [assertion]
          early skip of rewriting module: litellm.integrations.datadog.datadog_llm_obs [assertion]
          early skip of rewriting module: litellm.types.integrations.datadog_llm_obs [assertion]
          early skip of rewriting module: litellm.integrations.dynamodb [assertion]
          early skip of rewriting module: litellm.integrations.galileo [assertion]
          early skip of rewriting module: litellm.integrations.gcs_bucket [assertion]
          early skip of rewriting module: litellm.integrations.gcs_bucket.gcs_bucket [assertion]
          early skip of rewriting module: litellm.integrations.gcs_bucket.gcs_bucket_base [assertion]
          early skip of rewriting module: litellm.types.integrations.gcs_bucket [assertion]
          early skip of rewriting module: litellm.integrations.gcs_pubsub [assertion]
          early skip of rewriting module: litellm.integrations.gcs_pubsub.pub_sub [assertion]
          early skip of rewriting module: litellm.integrations.greenscale [assertion]
          early skip of rewriting module: litellm.integrations.helicone [assertion]
          early skip of rewriting module: litellm.integrations.humanloop [assertion]
          early skip of rewriting module: litellm.integrations.lago [assertion]
          early skip of rewriting module: litellm.integrations.langfuse [assertion]
          early skip of rewriting module: litellm.integrations.langfuse.langfuse [assertion]
          early skip of rewriting module: packaging [assertion]
          early skip of rewriting module: packaging.version [assertion]
          early skip of rewriting module: packaging._structures [assertion]
          early skip of rewriting module: litellm.types.integrations.langfuse [assertion]
          early skip of rewriting module: litellm.integrations.langfuse.langfuse_handler [assertion]
          early skip of rewriting module: litellm.integrations.langfuse.langfuse_prompt_management [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.asyncify [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.specialty_caches [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.specialty_caches.dynamic_logging_cache [assertion]
          early skip of rewriting module: litellm.integrations.langsmith [assertion]
          early skip of rewriting module: litellm.types.integrations.langsmith [assertion]
          early skip of rewriting module: litellm.integrations.literal_ai [assertion]
          early skip of rewriting module: litellm.integrations.logfire_logger [assertion]
          early skip of rewriting module: litellm.integrations.lunary [assertion]
          early skip of rewriting module: litellm.integrations.openmeter [assertion]
          early skip of rewriting module: litellm.integrations.opik [assertion]
          early skip of rewriting module: litellm.integrations.opik.opik [assertion]
          early skip of rewriting module: litellm.integrations.opik.utils [assertion]
          early skip of rewriting module: litellm.integrations.prometheus [assertion]
          early skip of rewriting module: litellm.integrations.prompt_layer [assertion]
          early skip of rewriting module: litellm.integrations.s3 [assertion]
          early skip of rewriting module: litellm.integrations.supabase [assertion]
          early skip of rewriting module: litellm.integrations.traceloop [assertion]
          early skip of rewriting module: litellm.integrations.weights_biases [assertion]
          early skip of rewriting module: wandb [assertion]
          early skip of rewriting module: wandb.sdk [assertion]
          early skip of rewriting module: wandb.sdk.data_types [assertion]
          early skip of rewriting module: wandb.sdk.data_types.trace_tree [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.initialize_dynamic_callback_params [assertion]
          early skip of rewriting module: litellm.proxy.enterprise [assertion]
          early skip of rewriting module: litellm.llms.custom_llm [assertion]
          early skip of rewriting module: litellm.llms.base [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.converse_handler [assertion]
          early skip of rewriting module: litellm.llms.bedrock.base_aws_llm [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.dd_tracing [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_handler [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.prompt_templates.factory [assertion]
          early skip of rewriting module: jinja2 [assertion]
          early skip of rewriting module: jinja2.bccache [assertion]
          early skip of rewriting module: jinja2.environment [assertion]
          early skip of rewriting module: markupsafe [assertion]
          early skip of rewriting module: markupsafe._speedups [assertion]
          early skip of rewriting module: jinja2.nodes [assertion]
          early skip of rewriting module: jinja2.utils [assertion]
          early skip of rewriting module: jinja2.compiler [assertion]
          early skip of rewriting module: jinja2.exceptions [assertion]
          early skip of rewriting module: jinja2.idtracking [assertion]
          early skip of rewriting module: jinja2.visitor [assertion]
          early skip of rewriting module: jinja2.optimizer [assertion]
          early skip of rewriting module: jinja2.defaults [assertion]
          early skip of rewriting module: jinja2.filters [assertion]
          early skip of rewriting module: jinja2.async_utils [assertion]
          early skip of rewriting module: jinja2.runtime [assertion]
          early skip of rewriting module: jinja2.tests [assertion]
          early skip of rewriting module: jinja2.lexer [assertion]
          early skip of rewriting module: jinja2._identifier [assertion]
          early skip of rewriting module: jinja2.parser [assertion]
          early skip of rewriting module: jinja2.loaders [assertion]
          early skip of rewriting module: jinja2.sandbox [assertion]
          early skip of rewriting module: litellm.types.llms.ollama [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.prompt_templates.image_handling [assertion]
          early skip of rewriting module: litellm.types.llms.cohere [assertion]
          early skip of rewriting module: litellm.llms.anthropic.chat [assertion]
          early skip of rewriting module: litellm.llms.anthropic.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.anthropic.common_utils [assertion]
          early skip of rewriting module: litellm.llms.anthropic.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.converse_transformation [assertion]
          early skip of rewriting module: litellm.llms.openai_like [assertion]
          early skip of rewriting module: litellm.llms.openai_like.chat [assertion]
          early skip of rewriting module: litellm.llms.openai_like.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.databricks.streaming_utils [assertion]
          early skip of rewriting module: litellm.llms.openai.chat [assertion]
          early skip of rewriting module: litellm.llms.openai.chat.gpt_transformation [assertion]
          early skip of rewriting module: litellm.llms.base_llm.base_model_iterator [assertion]
          early skip of rewriting module: litellm.llms.openai.openai [assertion]
          early skip of rewriting module: litellm.llms.openai.chat.o_series_transformation [assertion]
          early skip of rewriting module: litellm.llms.openai_like.common_utils [assertion]
          early skip of rewriting module: litellm.llms.openai_like.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.aiohttp_openai [assertion]
          early skip of rewriting module: litellm.llms.aiohttp_openai.chat [assertion]
          early skip of rewriting module: litellm.llms.aiohttp_openai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.galadriel [assertion]
          early skip of rewriting module: litellm.llms.galadriel.chat [assertion]
          early skip of rewriting module: litellm.llms.galadriel.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.github [assertion]
          early skip of rewriting module: litellm.llms.github.chat [assertion]
          early skip of rewriting module: litellm.llms.github.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.empower [assertion]
          early skip of rewriting module: litellm.llms.empower.chat [assertion]
          early skip of rewriting module: litellm.llms.empower.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.huggingface [assertion]
          early skip of rewriting module: litellm.llms.huggingface.chat [assertion]
          early skip of rewriting module: litellm.llms.huggingface.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.huggingface.common_utils [assertion]
          early skip of rewriting module: litellm.llms.oobabooga [assertion]
          early skip of rewriting module: litellm.llms.oobabooga.chat [assertion]
          early skip of rewriting module: litellm.llms.oobabooga.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.oobabooga.common_utils [assertion]
          early skip of rewriting module: litellm.llms.maritalk [assertion]
          early skip of rewriting module: litellm.llms.openrouter [assertion]
          early skip of rewriting module: litellm.llms.openrouter.chat [assertion]
          early skip of rewriting module: litellm.llms.openrouter.chat.transformation [assertion]
          early skip of rewriting module: litellm.types.llms.openrouter [assertion]
          early skip of rewriting module: litellm.llms.openrouter.common_utils [assertion]
          early skip of rewriting module: litellm.llms.groq [assertion]
          early skip of rewriting module: litellm.llms.groq.stt [assertion]
          early skip of rewriting module: litellm.llms.groq.stt.transformation [assertion]
          early skip of rewriting module: litellm.llms.anthropic.completion [assertion]
          early skip of rewriting module: litellm.llms.anthropic.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.triton [assertion]
          early skip of rewriting module: litellm.llms.triton.completion [assertion]
          early skip of rewriting module: litellm.llms.triton.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.triton.common_utils [assertion]
          early skip of rewriting module: litellm.llms.triton.embedding [assertion]
          early skip of rewriting module: litellm.llms.triton.embedding.transformation [assertion]
          early skip of rewriting module: litellm.llms.databricks.chat [assertion]
          early skip of rewriting module: litellm.llms.databricks.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.databricks.common_utils [assertion]
          early skip of rewriting module: litellm.llms.databricks.embed [assertion]
          early skip of rewriting module: litellm.llms.databricks.embed.transformation [assertion]
          early skip of rewriting module: litellm.llms.predibase [assertion]
          early skip of rewriting module: litellm.llms.predibase.chat [assertion]
          early skip of rewriting module: litellm.llms.predibase.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.predibase.common_utils [assertion]
          early skip of rewriting module: litellm.llms.replicate [assertion]
          early skip of rewriting module: litellm.llms.replicate.chat [assertion]
          early skip of rewriting module: litellm.llms.replicate.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.replicate.common_utils [assertion]
          early skip of rewriting module: litellm.llms.cohere [assertion]
          early skip of rewriting module: litellm.llms.cohere.completion [assertion]
          early skip of rewriting module: litellm.llms.cohere.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.cohere.common_utils [assertion]
          early skip of rewriting module: litellm.llms.snowflake [assertion]
          early skip of rewriting module: litellm.llms.snowflake.chat [assertion]
          early skip of rewriting module: litellm.llms.snowflake.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.cohere.rerank [assertion]
          early skip of rewriting module: litellm.llms.cohere.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.cohere.rerank_v2 [assertion]
          early skip of rewriting module: litellm.llms.cohere.rerank_v2.transformation [assertion]
          early skip of rewriting module: litellm.llms.azure_ai [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.rerank [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.infinity [assertion]
          early skip of rewriting module: litellm.llms.infinity.rerank [assertion]
          early skip of rewriting module: litellm.llms.infinity.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.infinity.rerank.common_utils [assertion]
          early skip of rewriting module: litellm.llms.jina_ai [assertion]
          early skip of rewriting module: litellm.llms.jina_ai.rerank [assertion]
          early skip of rewriting module: litellm.llms.jina_ai.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.clarifai [assertion]
          early skip of rewriting module: litellm.llms.clarifai.chat [assertion]
          early skip of rewriting module: litellm.llms.clarifai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.clarifai.common_utils [assertion]
          early skip of rewriting module: litellm.llms.ai21 [assertion]
          early skip of rewriting module: litellm.llms.ai21.chat [assertion]
          early skip of rewriting module: litellm.llms.ai21.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.anthropic.experimental_pass_through [assertion]
          early skip of rewriting module: litellm.llms.anthropic.experimental_pass_through.messages [assertion]
          early skip of rewriting module: litellm.llms.anthropic.experimental_pass_through.messages.transformation [assertion]
          early skip of rewriting module: litellm.llms.together_ai.chat [assertion]
          early skip of rewriting module: litellm.llms.together_ai.completion [assertion]
          early skip of rewriting module: litellm.llms.together_ai.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.openai.completion [assertion]
          early skip of rewriting module: litellm.llms.openai.completion.utils [assertion]
          early skip of rewriting module: litellm.llms.openai.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.cloudflare [assertion]
          early skip of rewriting module: litellm.llms.cloudflare.chat [assertion]
          early skip of rewriting module: litellm.llms.cloudflare.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.deprecated_providers [assertion]
          early skip of rewriting module: litellm.llms.deprecated_providers.palm [assertion]
          early skip of rewriting module: litellm.llms.nlp_cloud [assertion]
          early skip of rewriting module: litellm.llms.nlp_cloud.chat [assertion]
          early skip of rewriting module: litellm.llms.nlp_cloud.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.nlp_cloud.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.nlp_cloud.common_utils [assertion]
          early skip of rewriting module: litellm.llms.petals [assertion]
          early skip of rewriting module: litellm.llms.petals.completion [assertion]
          early skip of rewriting module: litellm.llms.petals.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.petals.common_utils [assertion]
          early skip of rewriting module: litellm.llms.deprecated_providers.aleph_alpha [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini.vertex_and_google_ai_studio_gemini [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.common_utils [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_llm_base [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini.transformation [assertion]
          early skip of rewriting module: litellm.types.files [assertion]
          early skip of rewriting module: litellm.llms.gemini.common_utils [assertion]
          early skip of rewriting module: litellm.llms.gemini.chat [assertion]
          early skip of rewriting module: litellm.llms.gemini.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_embeddings [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_embeddings.transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_embeddings.types [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.anthropic [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.anthropic.transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.llama3 [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.llama3.transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.ai21 [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.ai21.transformation [assertion]
          early skip of rewriting module: litellm.llms.ollama [assertion]
          early skip of rewriting module: litellm.llms.ollama.completion [assertion]
          early skip of rewriting module: litellm.llms.ollama.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.ollama.common_utils [assertion]
          early skip of rewriting module: litellm.llms.sagemaker [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.completion [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.common_utils [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.chat [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.ollama_chat [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_ai21_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.base_invoke_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_nova_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.anthropic_claude2_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.anthropic_claude3_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_cohere_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_llama_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_deepseek_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_mistral_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.chat.invoke_transformations.amazon_titan_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image.amazon_stability1_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image.amazon_stability3_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image.amazon_nova_canvas_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed.amazon_titan_g1_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed.amazon_titan_multimodal_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed.amazon_titan_v2_transformation [assertion]
          early skip of rewriting module: litellm.llms.cohere.chat [assertion]
          early skip of rewriting module: litellm.llms.cohere.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed.cohere_transformation [assertion]
          early skip of rewriting module: litellm.llms.cohere.embed [assertion]
          early skip of rewriting module: litellm.llms.cohere.embed.transformation [assertion]
          early skip of rewriting module: litellm.llms.openai.image_variations [assertion]
          early skip of rewriting module: litellm.llms.openai.image_variations.transformation [assertion]
          early skip of rewriting module: litellm.llms.deepinfra [assertion]
          early skip of rewriting module: litellm.llms.deepinfra.chat [assertion]
          early skip of rewriting module: litellm.llms.deepinfra.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.deepgram [assertion]
          early skip of rewriting module: litellm.llms.deepgram.audio_transcription [assertion]
          early skip of rewriting module: litellm.llms.deepgram.audio_transcription.transformation [assertion]
          early skip of rewriting module: litellm.llms.deepgram.common_utils [assertion]
          early skip of rewriting module: litellm.llms.topaz [assertion]
          early skip of rewriting module: litellm.llms.topaz.common_utils [assertion]
          early skip of rewriting module: litellm.llms.topaz.image_variations [assertion]
          early skip of rewriting module: litellm.llms.topaz.image_variations.transformation [assertion]
          early skip of rewriting module: litellm.llms.groq.chat [assertion]
          early skip of rewriting module: litellm.llms.groq.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.voyage [assertion]
          early skip of rewriting module: litellm.llms.voyage.embedding [assertion]
          early skip of rewriting module: litellm.llms.voyage.embedding.transformation [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.chat [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.mistral [assertion]
          early skip of rewriting module: litellm.llms.mistral.mistral_chat_transformation [assertion]
          early skip of rewriting module: litellm.types.llms.mistral [assertion]
          early skip of rewriting module: litellm.llms.openai.responses [assertion]
          early skip of rewriting module: litellm.llms.openai.responses.transformation [assertion]
          early skip of rewriting module: litellm.llms.openai.transcriptions [assertion]
          early skip of rewriting module: litellm.llms.openai.transcriptions.whisper_transformation [assertion]
          early skip of rewriting module: litellm.llms.openai.transcriptions.gpt_transformation [assertion]
          early skip of rewriting module: litellm.llms.openai.chat.gpt_audio_transformation [assertion]
          early skip of rewriting module: litellm.llms.nvidia_nim [assertion]
          early skip of rewriting module: litellm.llms.nvidia_nim.chat [assertion]
          early skip of rewriting module: litellm.llms.nvidia_nim.embed [assertion]
          early skip of rewriting module: litellm.llms.cerebras [assertion]
          early skip of rewriting module: litellm.llms.cerebras.chat [assertion]
          early skip of rewriting module: litellm.llms.sambanova [assertion]
          early skip of rewriting module: litellm.llms.sambanova.chat [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.chat [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.completion [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.common_utils [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.audio_transcription [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.audio_transcription.transformation [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.embed [assertion]
          early skip of rewriting module: litellm.llms.fireworks_ai.embed.fireworks_ai_transformation [assertion]
          early skip of rewriting module: litellm.llms.friendliai [assertion]
          early skip of rewriting module: litellm.llms.friendliai.chat [assertion]
          early skip of rewriting module: litellm.llms.friendliai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.jina_ai.embedding [assertion]
          early skip of rewriting module: litellm.llms.jina_ai.embedding.transformation [assertion]
          early skip of rewriting module: litellm.llms.xai [assertion]
          early skip of rewriting module: litellm.llms.xai.chat [assertion]
          early skip of rewriting module: litellm.llms.xai.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.xai.common_utils [assertion]
          early skip of rewriting module: litellm.llms.volcengine [assertion]
          early skip of rewriting module: litellm.llms.codestral [assertion]
          early skip of rewriting module: litellm.llms.codestral.completion [assertion]
          early skip of rewriting module: litellm.llms.codestral.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.azure.azure [assertion]
          early skip of rewriting module: litellm.llms.azure.chat [assertion]
          early skip of rewriting module: litellm.llms.azure.chat.gpt_transformation [assertion]
          early skip of rewriting module: litellm.types.llms.azure [assertion]
          early skip of rewriting module: litellm.llms.azure.completion [assertion]
          early skip of rewriting module: litellm.llms.azure.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.hosted_vllm [assertion]
          early skip of rewriting module: litellm.llms.hosted_vllm.chat [assertion]
          early skip of rewriting module: litellm.llms.hosted_vllm.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.litellm_proxy [assertion]
          early skip of rewriting module: litellm.llms.litellm_proxy.chat [assertion]
          early skip of rewriting module: litellm.llms.litellm_proxy.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.vllm [assertion]
          early skip of rewriting module: litellm.llms.vllm.completion [assertion]
          early skip of rewriting module: litellm.llms.vllm.completion.transformation [assertion]
          early skip of rewriting module: litellm.llms.deepseek.chat [assertion]
          early skip of rewriting module: litellm.llms.deepseek.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.lm_studio [assertion]
          early skip of rewriting module: litellm.llms.lm_studio.chat [assertion]
          early skip of rewriting module: litellm.llms.lm_studio.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.lm_studio.embed [assertion]
          early skip of rewriting module: litellm.llms.lm_studio.embed.transformation [assertion]
          early skip of rewriting module: litellm.llms.perplexity [assertion]
          early skip of rewriting module: litellm.llms.perplexity.chat [assertion]
          early skip of rewriting module: litellm.llms.perplexity.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.azure.chat.o_series_transformation [assertion]
          early skip of rewriting module: litellm.llms.watsonx [assertion]
          early skip of rewriting module: litellm.llms.watsonx.completion [assertion]
          early skip of rewriting module: litellm.llms.watsonx.completion.transformation [assertion]
          early skip of rewriting module: litellm.types.llms.watsonx [assertion]
          early skip of rewriting module: litellm.llms.watsonx.common_utils [assertion]
          early skip of rewriting module: litellm.llms.watsonx.chat [assertion]
          early skip of rewriting module: litellm.llms.watsonx.chat.transformation [assertion]
          early skip of rewriting module: litellm.llms.watsonx.embed [assertion]
          early skip of rewriting module: litellm.llms.watsonx.embed.transformation [assertion]
          early skip of rewriting module: litellm.main [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.health_check_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.mock_functions [assertion]
          early skip of rewriting module: litellm.realtime_api [assertion]
          early skip of rewriting module: litellm.realtime_api.main [assertion]
          early skip of rewriting module: litellm.llms.azure.realtime [assertion]
          early skip of rewriting module: litellm.llms.azure.realtime.handler [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.realtime_streaming [assertion]
          early skip of rewriting module: litellm.llms.openai.realtime [assertion]
          early skip of rewriting module: litellm.llms.openai.realtime.handler [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.fallback_utils [assertion]
          early skip of rewriting module: litellm.litellm_core_utils.streaming_chunk_builder_utils [assertion]
          early skip of rewriting module: litellm.llms.baseten [assertion]
          early skip of rewriting module: litellm.llms.azure.audio_transcriptions [assertion]
          early skip of rewriting module: litellm.llms.azure.chat.o_series_handler [assertion]
          early skip of rewriting module: litellm.llms.azure.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.embed [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.embed.handler [assertion]
          early skip of rewriting module: litellm.types.llms.azure_ai [assertion]
          early skip of rewriting module: litellm.llms.azure_ai.embed.cohere_transformation [assertion]
          early skip of rewriting module: litellm.llms.bedrock.embed.embedding [assertion]
          early skip of rewriting module: litellm.llms.cohere.embed.handler [assertion]
          early skip of rewriting module: litellm.llms.bedrock.image.image_handler [assertion]
          early skip of rewriting module: litellm.llms.codestral.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.custom_httpx.aiohttp_handler [assertion]
          early skip of rewriting module: litellm.llms.custom_httpx.llm_http_handler [assertion]
          early skip of rewriting module: litellm.responses.streaming_iterator [assertion]
          early skip of rewriting module: litellm.llms.databricks.embed.handler [assertion]
          early skip of rewriting module: litellm.llms.openai_like.embedding [assertion]
          early skip of rewriting module: litellm.llms.openai_like.embedding.handler [assertion]
          early skip of rewriting module: litellm.llms.groq.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.huggingface.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.ollama.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.oobabooga.chat.oobabooga [assertion]
          early skip of rewriting module: litellm.llms.openai.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.openai.image_variations.handler [assertion]
          early skip of rewriting module: litellm.llms.openai.transcriptions.handler [assertion]
          early skip of rewriting module: litellm.llms.petals.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.predibase.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.replicate.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.chat.handler [assertion]
          early skip of rewriting module: litellm.llms.sagemaker.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_non_gemini [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini_embeddings [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini_embeddings.batch_embed_content_handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.gemini_embeddings.batch_embed_content_transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.image_generation.image_generation_handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.multimodal_embeddings [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.multimodal_embeddings.embedding_handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.multimodal_embeddings.transformation [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.text_to_speech [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.text_to_speech.text_to_speech_handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_ai_partner_models.main [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_embeddings.embedding_handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_model_garden [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.vertex_model_garden.main [assertion]
          early skip of rewriting module: litellm.llms.vllm.completion.handler [assertion]
          early skip of rewriting module: litellm.llms.watsonx.chat.handler [assertion]
          early skip of rewriting module: litellm.budget_manager [assertion]
          early skip of rewriting module: litellm.proxy.proxy_cli [assertion]
          early skip of rewriting module: litellm.router [assertion]
          early skip of rewriting module: litellm.router_strategy [assertion]
          early skip of rewriting module: litellm.router_strategy.budget_limiter [assertion]
          early skip of rewriting module: litellm.router_strategy.tag_based_routing [assertion]
          early skip of rewriting module: litellm.router_utils.cooldown_callbacks [assertion]
          early skip of rewriting module: litellm.router_strategy.least_busy [assertion]
          early skip of rewriting module: litellm.router_strategy.lowest_cost [assertion]
          early skip of rewriting module: litellm.router_strategy.lowest_latency [assertion]
          early skip of rewriting module: litellm.router_strategy.lowest_tpm_rpm [assertion]
          early skip of rewriting module: litellm.router_strategy.lowest_tpm_rpm_v2 [assertion]
          early skip of rewriting module: litellm.router_strategy.base_routing_strategy [assertion]
          early skip of rewriting module: litellm.router_strategy.simple_shuffle [assertion]
          early skip of rewriting module: litellm.router_utils.add_retry_fallback_headers [assertion]
          early skip of rewriting module: litellm.router_utils.batch_utils [assertion]
          early skip of rewriting module: litellm.router_utils.client_initalization_utils [assertion]
          early skip of rewriting module: litellm.router_utils.clientside_credential_handler [assertion]
          early skip of rewriting module: litellm.router_utils.cooldown_cache [assertion]
          early skip of rewriting module: litellm.router_utils.cooldown_handlers [assertion]
          early skip of rewriting module: litellm.router_utils.router_callbacks [assertion]
          early skip of rewriting module: litellm.router_utils.router_callbacks.track_deployment_metrics [assertion]
          early skip of rewriting module: litellm.router_utils.fallback_event_handlers [assertion]
          early skip of rewriting module: litellm.router_utils.handle_error [assertion]
          early skip of rewriting module: litellm.router_utils.pre_call_checks [assertion]
          early skip of rewriting module: litellm.router_utils.pre_call_checks.prompt_caching_deployment_check [assertion]
          early skip of rewriting module: litellm.router_utils.prompt_caching_cache [assertion]
          early skip of rewriting module: litellm.scheduler [assertion]
          early skip of rewriting module: litellm.router_utils.pattern_match_deployments [assertion]
          early skip of rewriting module: litellm.assistants [assertion]
          early skip of rewriting module: litellm.assistants.main [assertion]
          early skip of rewriting module: litellm.llms.azure.assistants [assertion]
          early skip of rewriting module: litellm.assistants.utils [assertion]
          early skip of rewriting module: litellm.batches.main [assertion]
          early skip of rewriting module: litellm.llms.azure.batches [assertion]
          early skip of rewriting module: litellm.llms.azure.batches.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.batches [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.batches.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.batches.transformation [assertion]
          early skip of rewriting module: litellm.batch_completion [assertion]
          early skip of rewriting module: litellm.batch_completion.main [assertion]
          early skip of rewriting module: litellm.rerank_api [assertion]
          early skip of rewriting module: litellm.rerank_api.main [assertion]
          early skip of rewriting module: litellm.llms.bedrock.rerank [assertion]
          early skip of rewriting module: litellm.llms.bedrock.rerank.handler [assertion]
          early skip of rewriting module: litellm.llms.bedrock.rerank.transformation [assertion]
          early skip of rewriting module: litellm.llms.together_ai.rerank [assertion]
          early skip of rewriting module: litellm.llms.together_ai.rerank.handler [assertion]
          early skip of rewriting module: litellm.llms.together_ai.rerank.transformation [assertion]
          early skip of rewriting module: litellm.rerank_api.rerank_utils [assertion]
          early skip of rewriting module: litellm.llms.anthropic.experimental_pass_through.messages.handler [assertion]
          early skip of rewriting module: litellm.types.llms.anthropic_messages [assertion]
          early skip of rewriting module: litellm.types.llms.anthropic_messages.anthropic_response [assertion]
          early skip of rewriting module: litellm.responses.main [assertion]
          early skip of rewriting module: litellm.fine_tuning [assertion]
          early skip of rewriting module: litellm.fine_tuning.main [assertion]
          early skip of rewriting module: litellm.llms.azure.fine_tuning [assertion]
          early skip of rewriting module: litellm.llms.azure.fine_tuning.handler [assertion]
          early skip of rewriting module: litellm.llms.openai.fine_tuning [assertion]
          early skip of rewriting module: litellm.llms.openai.fine_tuning.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.fine_tuning [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.fine_tuning.handler [assertion]
          early skip of rewriting module: litellm.types.fine_tuning [assertion]
          early skip of rewriting module: litellm.files [assertion]
          early skip of rewriting module: litellm.files.main [assertion]
          early skip of rewriting module: litellm.llms.azure.files [assertion]
          early skip of rewriting module: litellm.llms.azure.files.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.files [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.files.handler [assertion]
          early skip of rewriting module: litellm.llms.vertex_ai.files.transformation [assertion]
          early skip of rewriting module: litellm.types.adapter [assertion]
          early skip of rewriting module: litellm.anthropic_interface [assertion]
          early skip of rewriting module: litellm.anthropic_interface.messages [assertion]
          early skip of rewriting module: litellm.types.llms.custom_llm [assertion]
          early skip of rewriting module: logprob_ranker.logprob_ranker.utils [assertion]
          early skip of rewriting module: logprob_ranker.logprob_ranker.models [assertion]
          early skip of rewriting module: logprob_ranker.tests [assertion]
          find_module called for: logprob_ranker.tests.conftest [assertion]
          rewriting conftest file: '/home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py' [assertion]
          found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py [assertion]
          loading conftestmodule <module 'logprob_ranker.tests.conftest' from '/home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py'> [pluginmanage]
            pytest_plugin_registered [hook]
                plugin: <module 'logprob_ranker.tests.conftest' from '/home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py'>
                manager: <_pytest.config.PytestPluginManager object at 0x7f64510b5370>
            finish pytest_plugin_registered --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
                path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
                  path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
              finish pytest_pycollect_makemodule --> <Package tests> [hook]
            finish pytest_collect_file --> [<Package tests>] [hook]
        finish pytest_make_collect_report --> <CollectReport '' lenresult=3 outcome='passed'> [hook]
        pytest_collectreport [hook]
            report: <CollectReport '' lenresult=3 outcome='passed'>
        finish pytest_collectreport --> [] [hook]
    genitems <Package logprob_ranker> [collection]
      pytest_collectstart [hook]
          collector: <Package logprob_ranker>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Package logprob_ranker>
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.coverage
            path: /home/tom/git/logprob-ranker/logprob_ranker/.coverage
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/.coverage
            path: /home/tom/git/logprob-ranker/logprob_ranker/.coverage
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.gitignore
            path: /home/tom/git/logprob-ranker/logprob_ranker/.gitignore
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/.gitignore
            path: /home/tom/git/logprob-ranker/logprob_ranker/.gitignore
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/CHANGELOG.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/CHANGELOG.md
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/CHANGELOG.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/CHANGELOG.md
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/DEVELOPMENT_NOTES.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/DEVELOPMENT_NOTES.md
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/DEVELOPMENT_NOTES.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/DEVELOPMENT_NOTES.md
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/LICENSE
            path: /home/tom/git/logprob-ranker/logprob_ranker/LICENSE
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/LICENSE
            path: /home/tom/git/logprob-ranker/logprob_ranker/LICENSE
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/MANIFEST.in
            path: /home/tom/git/logprob-ranker/logprob_ranker/MANIFEST.in
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/MANIFEST.in
            path: /home/tom/git/logprob-ranker/logprob_ranker/MANIFEST.in
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/PUBLISHING.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/PUBLISHING.md
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/PUBLISHING.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/PUBLISHING.md
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/pyproject.toml
            path: /home/tom/git/logprob-ranker/logprob_ranker/pyproject.toml
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/pyproject.toml
            path: /home/tom/git/logprob-ranker/logprob_ranker/pyproject.toml
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/run-tests.sh
            path: /home/tom/git/logprob-ranker/logprob_ranker/run-tests.sh
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/run-tests.sh
            path: /home/tom/git/logprob-ranker/logprob_ranker/run-tests.sh
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/uv.lock
            path: /home/tom/git/logprob-ranker/logprob_ranker/uv.lock
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/uv.lock
            path: /home/tom/git/logprob-ranker/logprob_ranker/uv.lock
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.pytest_cache
            path: /home/tom/git/logprob-ranker/logprob_ranker/.pytest_cache
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/.venv
            path: /home/tom/git/logprob-ranker/logprob_ranker/.venv
        finish pytest_ignore_collect --> True [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/build
            path: /home/tom/git/logprob-ranker/logprob_ranker/build
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs
            path: /home/tom/git/logprob-ranker/logprob_ranker/docs
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/docs/api_usage.md
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/docs/multi_provider_support.md
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/custom_llm_adapter.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/litellm_example.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/multi_provider_example.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/openai_example.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_example.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/simple_openai_example.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/code_generation.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/creative_writing.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/customer_service.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/marketing_copy.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/product_ideas.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
            path: /home/tom/git/logprob-ranker/logprob_ranker/examples/criteria_templates/technical_explanation.json
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__init__.py
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/PKG-INFO
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/SOURCES.txt
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/dependency_links.txt
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/entry_points.txt
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/requires.txt
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker.egg-info/top_level.txt
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests
        finish pytest_ignore_collect --> None [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/__init__.py
        finish pytest_ignore_collect --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/__init__.py' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/__init__.py' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Package logprob_ranker> [collection]
      pytest_collectstart [hook]
          collector: <Package logprob_ranker>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Package logprob_ranker>
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__main__.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__main__.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__main__.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/__main__.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/cli.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/cli.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/cli.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/cli.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/models.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/models.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/models.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/models.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/ranker.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/ranker.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/utils.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/utils.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package logprob_ranker>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/utils.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/logprob_ranker/utils.py
        finish pytest_collect_file --> [] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/logprob_ranker/__init__.py' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/logprob_ranker/__init__.py' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Package tests> [collection]
      pytest_collectstart [hook]
          collector: <Package tests>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Package tests>
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/README.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/README.md
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/README.md
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/README.md
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/conftest.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/run_async_tests.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/run_async_tests.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/run_async_tests.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/run_async_tests.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py
          finish pytest_pycollect_makemodule --> <Module test_async_basic.py> [hook]
        finish pytest_collect_file --> [<Module test_async_basic.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py
          finish pytest_pycollect_makemodule --> <Module test_cli.py> [hook]
        finish pytest_collect_file --> [<Module test_cli.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py
          finish pytest_pycollect_makemodule --> <Module test_litellm_adapter.py> [hook]
        finish pytest_collect_file --> [<Module test_litellm_adapter.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py
          finish pytest_pycollect_makemodule --> <Module test_litellm_basic.py> [hook]
        finish pytest_collect_file --> [<Module test_litellm_basic.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py
          finish pytest_pycollect_makemodule --> <Module test_litellm_functionality.py> [hook]
        finish pytest_collect_file --> [<Module test_litellm_functionality.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py
          finish pytest_pycollect_makemodule --> <Module test_logprob_ranker.py> [hook]
        finish pytest_collect_file --> [<Module test_logprob_ranker.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py
          finish pytest_pycollect_makemodule --> <Module test_ranker.py> [hook]
        finish pytest_collect_file --> [<Module test_ranker.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            collection_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Package tests>
            file_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
            path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
          pytest_pycollect_makemodule [hook]
              parent: <Package tests>
              module_path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
              path: /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py
          finish pytest_pycollect_makemodule --> <Module test_utils.py> [hook]
        finish pytest_collect_file --> [<Module test_utils.py>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/__init__.py' lenresult=8 outcome='passed'> [hook]
    genitems <Module test_async_basic.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_async_basic.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_async_basic.py>
      find_module called for: logprob_ranker.tests.test_async_basic [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: pytest
            obj: <module 'pytest' from '/usr/local/lib/python3.8/dist-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: ModelResponse
            obj: <class 'litellm.types.utils.ModelResponse'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: LiteLLMAdapter
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LiteLLMAdapter'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: mock_env_fixture
            obj: <function mock_env_fixture at 0x7f6449a28c10>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: __warningregistry__
            obj: {'version': 6272, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 59): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 80): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 100): True}
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: test_simple_generation
            obj: <function test_simple_generation at 0x7f6449a28af0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499c2820>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simple_generation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: test_error_handling
            obj: <function test_error_handling at 0x7f6449a28a60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499c2940>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_error_handling>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_async_basic.py>
            name: test_concurrent_tasks
            obj: <function test_concurrent_tasks at 0x7f6449a28940>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499c2a30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_concurrent_tasks>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_async_basic.py' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_simple_generation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simple_generation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_error_handling> [collection]
      pytest_itemcollected [hook]
          item: <Function test_error_handling>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_concurrent_tasks> [collection]
      pytest_itemcollected [hook]
          item: <Function test_concurrent_tasks>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_async_basic.py' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_cli.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_cli.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_cli.py>
      find_module called for: logprob_ranker.tests.test_cli [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py [assertion]
      early skip of rewriting module: logprob_ranker.logprob_ranker.cli [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: os
            obj: <module 'os' from '/usr/lib/python3.8/os.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: sys
            obj: <module 'sys' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: StringIO
            obj: <class '_io.StringIO'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: mock_open
            obj: <function mock_open at 0x7f6450f7e310>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: ANY
            obj: <ANY>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: pytest
            obj: <module 'pytest' from '/usr/local/lib/python3.8/dist-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: _PARENT_DIR
            obj: /home/tom/git/logprob-ranker/logprob_ranker
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: setup_parser
            obj: <function setup_parser at 0x7f6449a3bf70>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: load_template_from_file
            obj: <function load_template_from_file at 0x7f64499dc310>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: get_model_from_provider
            obj: <function get_model_from_provider at 0x7f64499dc3a0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: on_output_generated
            obj: <function on_output_generated at 0x7f64499dc430>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: run_rank_command
            obj: <function run_rank_command at 0x7f64499dc4c0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: print_provider_help
            obj: <function print_provider_help at 0x7f64499dc550>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: main
            obj: <function main at 0x7f64499dc5e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: AttributeScore
            obj: <class 'logprob_ranker.logprob_ranker.models.AttributeScore'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: mock_args_rank
            obj: <function mock_args_rank at 0x7f64499dc670>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_setup_parser
            obj: <function test_setup_parser at 0x7f64499dc700>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de250>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_setup_parser>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_load_template_from_file
            obj: <function test_load_template_from_file at 0x7f64499dc790>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de5e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_load_template_from_file>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_get_model_from_provider
            obj: <function test_get_model_from_provider at 0x7f64499dc820>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de6d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_get_model_from_provider>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_on_output_generated
            obj: <function test_on_output_generated at 0x7f64499dc8b0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de7c0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_on_output_generated>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_main_rank_command
            obj: <function test_main_rank_command at 0x7f64499dca60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de8b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_main_rank_command>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_main_no_command
            obj: <function test_main_no_command at 0x7f64499dcaf0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499de9a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_main_no_command>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_print_provider_help
            obj: <function test_print_provider_help at 0x7f64499dcb80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499dea90>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_print_provider_help>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: __warningregistry__
            obj: {'version': 6272, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 209): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 250): True}
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_run_rank_command_no_output_file
            obj: <function test_run_rank_command_no_output_file at 0x7f64499dcdc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499deb80>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_rank_command_no_output_file>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cli.py>
            name: test_run_rank_command_with_output_file
            obj: <function test_run_rank_command_with_output_file at 0x7f64499e0160>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499c2160>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_rank_command_with_output_file>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_cli.py' lenresult=9 outcome='passed'> [hook]
    genitems <Function test_setup_parser> [collection]
      pytest_itemcollected [hook]
          item: <Function test_setup_parser>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_load_template_from_file> [collection]
      pytest_itemcollected [hook]
          item: <Function test_load_template_from_file>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_get_model_from_provider> [collection]
      pytest_itemcollected [hook]
          item: <Function test_get_model_from_provider>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_on_output_generated> [collection]
      pytest_itemcollected [hook]
          item: <Function test_on_output_generated>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_main_rank_command> [collection]
      pytest_itemcollected [hook]
          item: <Function test_main_rank_command>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_main_no_command> [collection]
      pytest_itemcollected [hook]
          item: <Function test_main_no_command>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_print_provider_help> [collection]
      pytest_itemcollected [hook]
          item: <Function test_print_provider_help>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_rank_command_no_output_file> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_rank_command_no_output_file>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_rank_command_with_output_file> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_rank_command_with_output_file>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_cli.py' lenresult=9 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_litellm_adapter.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_litellm_adapter.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_litellm_adapter.py>
      find_module called for: logprob_ranker.tests.test_litellm_adapter [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_adapter.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: asyncio
            obj: <module 'asyncio' from '/usr/lib/python3.8/asyncio/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: json
            obj: <module 'json' from '/usr/lib/python3.8/json/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: aiohttp
            obj: <module 'aiohttp' from '/usr/local/lib/python3.8/dist-packages/aiohttp/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: os
            obj: <module 'os' from '/usr/lib/python3.8/os.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: unittest
            obj: <module 'unittest' from '/usr/lib/python3.8/unittest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: ModelResponse
            obj: <class 'litellm.types.utils.ModelResponse'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: Choices
            obj: <class 'litellm.types.utils.Choices'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: OpenAIChoiceLogprobs
            obj: <class 'openai.types.chat.chat_completion.ChoiceLogprobs'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: ChatCompletionTokenLogprob
            obj: <class 'openai.types.chat.chat_completion_token_logprob.ChatCompletionTokenLogprob'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: LiteLLMAdapter
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LiteLLMAdapter'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: LogprobsNotAvailableError
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LogprobsNotAvailableError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: MalformedLogprobsError
            obj: <class 'logprob_ranker.logprob_ranker.ranker.MalformedLogprobsError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: LLMGenerationError
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LLMGenerationError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_lp_item1
            obj: <MagicMock spec='ChatCompletionTokenLogprob' id='140068708542784'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_lp_item2
            obj: <MagicMock spec='ChatCompletionTokenLogprob' id='140068708622144'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_logprobs_data
            obj: <MagicMock name='mock.logprobs' spec='ChoiceLogprobs' id='140068708625808'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_message
            obj: <MagicMock name='mock.message' id='140068708622288'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_choice
            obj: <MagicMock spec='Choices' id='140068708188368'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: mock_anthropic_response
            obj: <MagicMock spec='ModelResponse' id='140068708626288'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: _mock_anthropic_dump_json
            obj: <function _mock_anthropic_dump_json at 0x7f644e5dc0d0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: TestLiteLLMAdapter
            obj: <class 'logprob_ranker.tests.test_litellm_adapter.TestLiteLLMAdapter'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestLiteLLMAdapter> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_adapter.py>
            name: run_async_test
            obj: <function run_async_test at 0x7f644e5dc040>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_litellm_adapter.py' lenresult=1 outcome='passed'> [hook]
    genitems <UnitTestCase TestLiteLLMAdapter> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestLiteLLMAdapter>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestLiteLLMAdapter>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter' lenresult=8 outcome='passed'> [hook]
    genitems <TestCaseFunction test_anthropic_integration> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_anthropic_integration>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_create_chat_completion> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_create_chat_completion>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_empty_logprobs_content_mocked> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_empty_logprobs_content_mocked>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_initialization> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_malformed_logprobs_item_mocked> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_malformed_logprobs_item_mocked>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_missing_logprobs_attribute_mocked> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_rank_outputs> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_rank_outputs>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_real_logprobs_output> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_real_logprobs_output>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter' lenresult=8 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_litellm_adapter.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_litellm_basic.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_litellm_basic.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_litellm_basic.py>
      find_module called for: logprob_ranker.tests.test_litellm_basic [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_basic.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: os
            obj: <module 'os' from '/usr/lib/python3.8/os.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: sys
            obj: <module 'sys' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: unittest
            obj: <module 'unittest' from '/usr/lib/python3.8/unittest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: LiteLLMAdapter
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LiteLLMAdapter'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_basic.py>
            name: TestLiteLLMBasic
            obj: <class 'logprob_ranker.tests.test_litellm_basic.TestLiteLLMBasic'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestLiteLLMBasic> [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_litellm_basic.py' lenresult=1 outcome='passed'> [hook]
    genitems <UnitTestCase TestLiteLLMBasic> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestLiteLLMBasic>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestLiteLLMBasic>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic' lenresult=3 outcome='passed'> [hook]
    genitems <TestCaseFunction test_anthropic_initialization> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_anthropic_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_custom_initialization> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_custom_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_openai_initialization> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_openai_initialization>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_litellm_basic.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_litellm_functionality.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_litellm_functionality.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_litellm_functionality.py>
      find_module called for: logprob_ranker.tests.test_litellm_functionality [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: os
            obj: <module 'os' from '/usr/lib/python3.8/os.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: sys
            obj: <module 'sys' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: pytest
            obj: <module 'pytest' from '/usr/local/lib/python3.8/dist-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: LiteLLMAdapter
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LiteLLMAdapter'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: AttributeScore
            obj: <class 'logprob_ranker.logprob_ranker.models.AttributeScore'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: config
            obj: <function config at 0x7f64499e0430>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: __warningregistry__
            obj: {'version': 6272, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 32): True}
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: test_simple_rank
            obj: <function test_simple_rank at 0x7f64499ac040>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499e53a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simple_rank>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_litellm_functionality.py>
            name: test_sync_wrapper
            obj: <function test_sync_wrapper at 0x7f64499ac0d0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499e5dc0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_sync_wrapper>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_litellm_functionality.py' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_simple_rank> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simple_rank>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_sync_wrapper> [collection]
      pytest_itemcollected [hook]
          item: <Function test_sync_wrapper>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_litellm_functionality.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_logprob_ranker.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_logprob_ranker.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_logprob_ranker.py>
      find_module called for: logprob_ranker.tests.test_logprob_ranker [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_logprob_ranker.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: asyncio
            obj: <module 'asyncio' from '/usr/lib/python3.8/asyncio/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: unittest
            obj: <module 'unittest' from '/usr/lib/python3.8/unittest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: List
            obj: typing.List
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: Dict
            obj: typing.Dict
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: Any
            obj: typing.Any
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: Optional
            obj: typing.Optional
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: LogProbRanker
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LogProbRanker'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: LLMGenerationError
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LLMGenerationError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: run_async_test
            obj: <function run_async_test at 0x7f6451824c10>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: MinimalConcreteRanker
            obj: <class 'logprob_ranker.tests.test_logprob_ranker.MinimalConcreteRanker'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: TestLogProbRankerErrorHandling
            obj: <class 'logprob_ranker.tests.test_logprob_ranker.TestLogProbRankerErrorHandling'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestLogProbRankerErrorHandling> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_logprob_ranker.py>
            name: TestLogProbRankerScoring
            obj: <class 'logprob_ranker.tests.test_logprob_ranker.TestLogProbRankerScoring'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestLogProbRankerScoring> [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py' lenresult=2 outcome='passed'> [hook]
    genitems <UnitTestCase TestLogProbRankerErrorHandling> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestLogProbRankerErrorHandling>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestLogProbRankerErrorHandling>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling' lenresult=4 outcome='passed'> [hook]
    genitems <TestCaseFunction test_gen_eval_handles_evaluation_error> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_gen_eval_handles_generation_error> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_gen_eval_handles_generation_error>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_rank_outputs_handles_all_failures> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_rank_outputs_handles_all_failures>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_rank_outputs_handles_some_failures> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_rank_outputs_handles_some_failures>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling' lenresult=4 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <UnitTestCase TestLogProbRankerScoring> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestLogProbRankerScoring>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestLogProbRankerScoring>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring' lenresult=1 outcome='passed'> [hook]
    genitems <TestCaseFunction test_simple_successful_scoring> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_simple_successful_scoring>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_logprob_ranker.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_ranker.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_ranker.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_ranker.py>
      find_module called for: logprob_ranker.tests.test_ranker [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: os
            obj: <module 'os' from '/usr/lib/python3.8/os.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: sys
            obj: <module 'sys' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: unittest
            obj: <module 'unittest' from '/usr/lib/python3.8/unittest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: MagicMock
            obj: <class 'unittest.mock.MagicMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: AsyncMock
            obj: <class 'unittest.mock.AsyncMock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: patch
            obj: <function patch at 0x7f6450f76ca0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: pytest
            obj: <module 'pytest' from '/usr/local/lib/python3.8/dist-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: LLMGenerationError
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LLMGenerationError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: LogProbConfig
            obj: <class 'logprob_ranker.logprob_ranker.models.LogProbConfig'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: LogProbRanker
            obj: <class 'logprob_ranker.logprob_ranker.ranker.LogProbRanker'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: sort_ranked_outputs
            obj: <function sort_ranked_outputs at 0x7f6449a3b5e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: ConcreteTestRanker
            obj: <class 'logprob_ranker.tests.test_ranker.ConcreteTestRanker'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: create_mock_litellm_response
            obj: <function create_mock_litellm_response at 0x7f64499b3430>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: config
            obj: <function config at 0x7f64499b35e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: ranker
            obj: <function ranker at 0x7f64499b3700>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_initialization
            obj: <function test_initialization at 0x7f64499b3790>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941310>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_initialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_initialization_default_config
            obj: <function test_initialization_default_config at 0x7f64499b3820>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941430>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_initialization_default_config>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_initialization_with_callback
            obj: <function test_initialization_with_callback at 0x7f64499b38b0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941580>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_initialization_with_callback>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: __warningregistry__
            obj: {'version': 6272, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 100): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 140): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 186): True, ('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html', <class 'pytest.PytestUnknownMarkWarning'>, 214): True}
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_parse_evaluation
            obj: <function test_parse_evaluation at 0x7f64499b3940>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941670>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parse_evaluation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_generate_and_evaluate_output
            obj: <function test_generate_and_evaluate_output at 0x7f64499b3af0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f64499412b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_generate_and_evaluate_output>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_generate_and_evaluate_output_failure
            obj: <function test_generate_and_evaluate_output_failure at 0x7f64499b3ca0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941850>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_generate_and_evaluate_output_failure>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_rank_outputs
            obj: <function test_rank_outputs at 0x7f64499b3e50>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941a00>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_rank_outputs>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_rank_outputs_sync
            obj: <function test_rank_outputs_sync at 0x7f6449943040>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941ac0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_rank_outputs_sync>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_ranker.py>
            name: test_sort_ranked_outputs
            obj: <function test_sort_ranked_outputs at 0x7f64499430d0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x7f6449941c10>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_sort_ranked_outputs>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_ranker.py' lenresult=9 outcome='passed'> [hook]
    genitems <Function test_initialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_initialization_default_config> [collection]
      pytest_itemcollected [hook]
          item: <Function test_initialization_default_config>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_initialization_with_callback> [collection]
      pytest_itemcollected [hook]
          item: <Function test_initialization_with_callback>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parse_evaluation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parse_evaluation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_generate_and_evaluate_output> [collection]
      pytest_itemcollected [hook]
          item: <Function test_generate_and_evaluate_output>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_generate_and_evaluate_output_failure> [collection]
      pytest_itemcollected [hook]
          item: <Function test_generate_and_evaluate_output_failure>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_rank_outputs> [collection]
      pytest_itemcollected [hook]
          item: <Function test_rank_outputs>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_rank_outputs_sync> [collection]
      pytest_itemcollected [hook]
          item: <Function test_rank_outputs_sync>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_sort_ranked_outputs> [collection]
      pytest_itemcollected [hook]
          item: <Function test_sort_ranked_outputs>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_ranker.py' lenresult=9 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_utils.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_utils.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_utils.py>
      find_module called for: logprob_ranker.tests.test_utils [assertion]
      matched test file '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py' [assertion]
      found cached rewritten pyc for /home/tom/git/logprob-ranker/logprob_ranker/tests/test_utils.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/usr/local/lib/python3.8/dist-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: unittest
            obj: <module 'unittest' from '/usr/lib/python3.8/unittest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: parse_evaluation_json
            obj: <function parse_evaluation_json at 0x7f6449a28160>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: extract_template_attributes
            obj: <function extract_template_attributes at 0x7f6449a28700>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: format_evaluation_prompt
            obj: <function format_evaluation_prompt at 0x7f6449a3b550>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: sort_ranked_outputs
            obj: <function sort_ranked_outputs at 0x7f6449a3b5e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: serialize_ranked_output
            obj: <function serialize_ranked_output at 0x7f6449a3b670>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: deserialize_ranked_output
            obj: <function deserialize_ranked_output at 0x7f6449a3b700>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: EvaluationParseError
            obj: <class 'logprob_ranker.logprob_ranker.utils.EvaluationParseError'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: RankedOutput
            obj: <class 'logprob_ranker.logprob_ranker.models.RankedOutput'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: AttributeScore
            obj: <class 'logprob_ranker.logprob_ranker.models.AttributeScore'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: TestParseEvaluationJson
            obj: <class 'logprob_ranker.tests.test_utils.TestParseEvaluationJson'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestParseEvaluationJson> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: TestUtils
            obj: <class 'logprob_ranker.tests.test_utils.TestUtils'>
        finish pytest_pycollect_makeitem --> <UnitTestCase TestUtils> [hook]
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_utils.py' lenresult=2 outcome='passed'> [hook]
    genitems <UnitTestCase TestParseEvaluationJson> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestParseEvaluationJson>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestParseEvaluationJson>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson' lenresult=10 outcome='passed'> [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_direct> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_direct>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_in_markdown> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_invalid_json> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_multiple_json_objects> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_no_json> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_no_json>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_not_dict> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_not_dict>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_not_dict_in_text> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_parse_evaluation_json_with_text> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_parse_evaluation_json_with_text>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson' lenresult=10 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <UnitTestCase TestUtils> [collection]
      pytest_collectstart [hook]
          collector: <UnitTestCase TestUtils>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <UnitTestCase TestUtils>
      finish pytest_make_collect_report --> <CollectReport 'logprob_ranker/tests/test_utils.py::TestUtils' lenresult=23 outcome='passed'> [hook]
    genitems <TestCaseFunction test_deserialize_ranked_output_basic> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_deserialize_ranked_output_incorrect_types> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_deserialize_ranked_output_missing_required_fields> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_deserialize_ranked_output_with_extras> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_basic> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_case_insensitive> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_empty_json> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_empty_json>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_invalid_json> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_invalid_json>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_invalid_json_no_match> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_mixed_values> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_mixed_values>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_no_logprob> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_no_logprob>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_regex_fallback> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_extract_template_attributes_whitespace_variation> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_format_evaluation_prompt> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_format_evaluation_prompt>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_format_evaluation_prompt_empty_generated_text> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_format_evaluation_prompt_empty_template> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_serialize_ranked_output_basic> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_serialize_ranked_output_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_serialize_ranked_output_with_extras> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_serialize_ranked_output_with_extras>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_sort_ranked_outputs> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_sort_ranked_outputs>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_sort_ranked_outputs_empty_list> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
      finish pytest_itemcollected --> [] [hook]
    genitems <TestCaseFunction test_sort_ranked_outputs_single_item> [collection]
      pytest_itemcollected [hook]
          item: <TestCaseFunction test_sort_ranked_outputs_single_item>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_utils.py::TestUtils' lenresult=23 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/test_utils.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'logprob_ranker/tests/__init__.py' lenresult=8 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collection_modifyitems [hook]
          session: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          config: <_pytest.config.Config object at 0x7f645106a280>
          items: [<Function test_simple_generation>, <Function test_error_handling>, <Function test_concurrent_tasks>, <Function test_setup_parser>, <Function test_load_template_from_file>, <Function test_get_model_from_provider>, <Function test_on_output_generated>, <Function test_main_rank_command>, <Function test_main_no_command>, <Function test_print_provider_help>, <Function test_run_rank_command_no_output_file>, <Function test_run_rank_command_with_output_file>, <TestCaseFunction test_anthropic_integration>, <TestCaseFunction test_create_chat_completion>, <TestCaseFunction test_empty_logprobs_content_mocked>, <TestCaseFunction test_initialization>, <TestCaseFunction test_malformed_logprobs_item_mocked>, <TestCaseFunction test_missing_logprobs_attribute_mocked>, <TestCaseFunction test_rank_outputs>, <TestCaseFunction test_real_logprobs_output>, <TestCaseFunction test_anthropic_initialization>, <TestCaseFunction test_custom_initialization>, <TestCaseFunction test_openai_initialization>, <Function test_simple_rank>, <Function test_sync_wrapper>, <TestCaseFunction test_gen_eval_handles_evaluation_error>, <TestCaseFunction test_gen_eval_handles_generation_error>, <TestCaseFunction test_rank_outputs_handles_all_failures>, <TestCaseFunction test_rank_outputs_handles_some_failures>, <TestCaseFunction test_simple_successful_scoring>, <Function test_initialization>, <Function test_initialization_default_config>, <Function test_initialization_with_callback>, <Function test_parse_evaluation>, <Function test_generate_and_evaluate_output>, <Function test_generate_and_evaluate_output_failure>, <Function test_rank_outputs>, <Function test_rank_outputs_sync>, <Function test_sort_ranked_outputs>, <TestCaseFunction test_parse_evaluation_json_direct>, <TestCaseFunction test_parse_evaluation_json_in_markdown>, <TestCaseFunction test_parse_evaluation_json_invalid_json>, <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>, <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>, <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>, <TestCaseFunction test_parse_evaluation_json_no_json>, <TestCaseFunction test_parse_evaluation_json_not_dict>, <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>, <TestCaseFunction test_parse_evaluation_json_with_text>, <TestCaseFunction test_deserialize_ranked_output_basic>, <TestCaseFunction test_deserialize_ranked_output_incorrect_types>, <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>, <TestCaseFunction test_deserialize_ranked_output_with_extras>, <TestCaseFunction test_extract_template_attributes_basic>, <TestCaseFunction test_extract_template_attributes_case_insensitive>, <TestCaseFunction test_extract_template_attributes_empty_json>, <TestCaseFunction test_extract_template_attributes_invalid_json>, <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>, <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>, <TestCaseFunction test_extract_template_attributes_mixed_values>, <TestCaseFunction test_extract_template_attributes_no_logprob>, <TestCaseFunction test_extract_template_attributes_regex_fallback>, <TestCaseFunction test_extract_template_attributes_whitespace_variation>, <TestCaseFunction test_format_evaluation_prompt>, <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>, <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>, <TestCaseFunction test_format_evaluation_prompt_empty_template>, <TestCaseFunction test_serialize_ranked_output_basic>, <TestCaseFunction test_serialize_ranked_output_with_extras>, <TestCaseFunction test_sort_ranked_outputs>, <TestCaseFunction test_sort_ranked_outputs_empty_list>, <TestCaseFunction test_sort_ranked_outputs_single_item>]
      finish pytest_collection_modifyitems --> [] [hook]
      pytest_collection_finish [hook]
          session: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        pytest_report_collectionfinish [hook]
            config: <_pytest.config.Config object at 0x7f645106a280>
            items: [<Function test_simple_generation>, <Function test_error_handling>, <Function test_concurrent_tasks>, <Function test_setup_parser>, <Function test_load_template_from_file>, <Function test_get_model_from_provider>, <Function test_on_output_generated>, <Function test_main_rank_command>, <Function test_main_no_command>, <Function test_print_provider_help>, <Function test_run_rank_command_no_output_file>, <Function test_run_rank_command_with_output_file>, <TestCaseFunction test_anthropic_integration>, <TestCaseFunction test_create_chat_completion>, <TestCaseFunction test_empty_logprobs_content_mocked>, <TestCaseFunction test_initialization>, <TestCaseFunction test_malformed_logprobs_item_mocked>, <TestCaseFunction test_missing_logprobs_attribute_mocked>, <TestCaseFunction test_rank_outputs>, <TestCaseFunction test_real_logprobs_output>, <TestCaseFunction test_anthropic_initialization>, <TestCaseFunction test_custom_initialization>, <TestCaseFunction test_openai_initialization>, <Function test_simple_rank>, <Function test_sync_wrapper>, <TestCaseFunction test_gen_eval_handles_evaluation_error>, <TestCaseFunction test_gen_eval_handles_generation_error>, <TestCaseFunction test_rank_outputs_handles_all_failures>, <TestCaseFunction test_rank_outputs_handles_some_failures>, <TestCaseFunction test_simple_successful_scoring>, <Function test_initialization>, <Function test_initialization_default_config>, <Function test_initialization_with_callback>, <Function test_parse_evaluation>, <Function test_generate_and_evaluate_output>, <Function test_generate_and_evaluate_output_failure>, <Function test_rank_outputs>, <Function test_rank_outputs_sync>, <Function test_sort_ranked_outputs>, <TestCaseFunction test_parse_evaluation_json_direct>, <TestCaseFunction test_parse_evaluation_json_in_markdown>, <TestCaseFunction test_parse_evaluation_json_invalid_json>, <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>, <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>, <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>, <TestCaseFunction test_parse_evaluation_json_no_json>, <TestCaseFunction test_parse_evaluation_json_not_dict>, <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>, <TestCaseFunction test_parse_evaluation_json_with_text>, <TestCaseFunction test_deserialize_ranked_output_basic>, <TestCaseFunction test_deserialize_ranked_output_incorrect_types>, <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>, <TestCaseFunction test_deserialize_ranked_output_with_extras>, <TestCaseFunction test_extract_template_attributes_basic>, <TestCaseFunction test_extract_template_attributes_case_insensitive>, <TestCaseFunction test_extract_template_attributes_empty_json>, <TestCaseFunction test_extract_template_attributes_invalid_json>, <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>, <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>, <TestCaseFunction test_extract_template_attributes_mixed_values>, <TestCaseFunction test_extract_template_attributes_no_logprob>, <TestCaseFunction test_extract_template_attributes_regex_fallback>, <TestCaseFunction test_extract_template_attributes_whitespace_variation>, <TestCaseFunction test_format_evaluation_prompt>, <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>, <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>, <TestCaseFunction test_format_evaluation_prompt_empty_template>, <TestCaseFunction test_serialize_ranked_output_basic>, <TestCaseFunction test_serialize_ranked_output_with_extras>, <TestCaseFunction test_sort_ranked_outputs>, <TestCaseFunction test_sort_ranked_outputs_empty_list>, <TestCaseFunction test_sort_ranked_outputs_single_item>]
            start_path: /home/tom/git/logprob-ranker
            startdir: /home/tom/git/logprob-ranker
        finish pytest_report_collectionfinish --> [] [hook]
      finish pytest_collection_finish --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestConfigWarning('Unknown config option: asyncio_mode\n'), category : 'PytestConfigWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/config/__init__.py', lineno : 1302, line : None}
          when: config
          nodeid: 
          location: ('/usr/local/lib/python3.8/dist-packages/_pytest/config/__init__.py', 1302, '_validate_config_options')
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestConfigWarning('Unknown config option: plugins\n'), category : 'PytestConfigWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/config/__init__.py', lineno : 1302, line : None}
          when: config
          nodeid: 
          location: ('/usr/local/lib/python3.8/dist-packages/_pytest/config/__init__.py', 1302, '_validate_config_options')
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py', lineno : 59, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py', lineno : 80, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_async_basic.py', lineno : 100, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py', lineno : 209, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_cli.py', lineno : 250, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_litellm_functionality.py', lineno : 32, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py', lineno : 100, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py', lineno : 140, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py', lineno : 186, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
      pytest_warning_recorded [hook]
          warning_message: {message : PytestUnknownMarkWarning('Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html'), category : 'PytestUnknownMarkWarning', filename : '/home/tom/git/logprob-ranker/logprob_ranker/tests/test_ranker.py', lineno : 214, line : None}
          nodeid: 
          when: collect
          location: None
      finish pytest_warning_recorded --> [] [hook]
    finish pytest_collection --> None [hook]
    pytest_runtestloop [hook]
        session: <Session logprob-ranker exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=72>
      pytest_runtest_protocol [hook]
          item: <Function test_simple_generation>
          nextitem: <Function test_error_handling>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_simple_generation
            location: ('logprob_ranker/tests/test_async_basic.py', 58, 'test_simple_generation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simple_generation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_simple_generation>>
          finish pytest_fixture_setup --> (<MagicMock name='litellm' id='140068708039264'>, LogProbConfig(temperature=0.7, max_tokens=100, top_p=1.0, logprobs=False, top_logprobs=None, num_variants=1, thread_count=1, evaluation_top_logprobs=None, template='{"test": LOGPROB_TRUE}', system_prompt='You are a creative assistant that provides a single concise response.', evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).'), <MagicMock spec='ModelResponse' id='140068707374176'>, <MagicMock spec='ModelResponse' id='140068707512176'>) [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_simple_generation>>
          finish pytest_fixture_setup --> <async_generator object aiohttp_session at 0x7f644991db80> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simple_generation>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simple_generation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simple_generation>
        pytest_runtest_makereport [hook]
            item: <Function test_simple_generation>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simple_generation>
            nextitem: <Function test_error_handling>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_simple_generation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_simple_generation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simple_generation>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_simple_generation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_simple_generation
            location: ('logprob_ranker/tests/test_async_basic.py', 58, 'test_simple_generation')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_async_basic.py::test_simple_generation
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_error_handling>
          nextitem: <Function test_concurrent_tasks>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_error_handling
            location: ('logprob_ranker/tests/test_async_basic.py', 79, 'test_error_handling')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_error_handling>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_error_handling>>
          finish pytest_fixture_setup --> (<MagicMock name='litellm' id='140068707544320'>, LogProbConfig(temperature=0.7, max_tokens=100, top_p=1.0, logprobs=False, top_logprobs=None, num_variants=1, thread_count=1, evaluation_top_logprobs=None, template='{"test": LOGPROB_TRUE}', system_prompt='You are a creative assistant that provides a single concise response.', evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).'), <MagicMock spec='ModelResponse' id='140068706876960'>, <MagicMock spec='ModelResponse' id='140068707015024'>) [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_error_handling>>
          finish pytest_fixture_setup --> <async_generator object aiohttp_session at 0x7f644991dca0> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_error_handling>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_error_handling>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_error_handling>
        pytest_runtest_makereport [hook]
            item: <Function test_error_handling>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_error_handling>
            nextitem: <Function test_concurrent_tasks>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_error_handling>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_error_handling>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_error_handling>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_error_handling' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_error_handling
            location: ('logprob_ranker/tests/test_async_basic.py', 79, 'test_error_handling')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_async_basic.py::test_error_handling
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_concurrent_tasks>
          nextitem: <Function test_setup_parser>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks
            location: ('logprob_ranker/tests/test_async_basic.py', 99, 'test_concurrent_tasks')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_concurrent_tasks>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_concurrent_tasks>>
          finish pytest_fixture_setup --> (<MagicMock name='litellm' id='140068707055168'>, LogProbConfig(temperature=0.7, max_tokens=100, top_p=1.0, logprobs=False, top_logprobs=None, num_variants=1, thread_count=1, evaluation_top_logprobs=None, template='{"test": LOGPROB_TRUE}', system_prompt='You are a creative assistant that provides a single concise response.', evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).'), <MagicMock spec='ModelResponse' id='140068706612176'>, <MagicMock spec='ModelResponse' id='140068708037728'>) [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_concurrent_tasks>>
          finish pytest_fixture_setup --> <async_generator object aiohttp_session at 0x7f64498b9160> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_concurrent_tasks>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_concurrent_tasks>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_concurrent_tasks>
        pytest_runtest_makereport [hook]
            item: <Function test_concurrent_tasks>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_concurrent_tasks>
            nextitem: <Function test_setup_parser>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='aiohttp_session' scope='function' baseid='logprob_ranker/tests'>
              request: <SubRequest 'aiohttp_session' for <Function test_concurrent_tasks>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_env_fixture' scope='function' baseid='logprob_ranker/tests/test_async_basic.py'>
              request: <SubRequest 'mock_env_fixture' for <Function test_concurrent_tasks>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_concurrent_tasks>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks
            location: ('logprob_ranker/tests/test_async_basic.py', 99, 'test_concurrent_tasks')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_async_basic.py::test_concurrent_tasks
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_setup_parser>
          nextitem: <Function test_load_template_from_file>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_setup_parser
            location: ('logprob_ranker/tests/test_cli.py', 56, 'test_setup_parser')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_setup_parser>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_setup_parser>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_setup_parser>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_setup_parser>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_setup_parser>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_setup_parser>
            nextitem: <Function test_load_template_from_file>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_setup_parser>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_setup_parser' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_setup_parser
            location: ('logprob_ranker/tests/test_cli.py', 56, 'test_setup_parser')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_load_template_from_file>
          nextitem: <Function test_get_model_from_provider>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_load_template_from_file
            location: ('logprob_ranker/tests/test_cli.py', 82, 'test_load_template_from_file')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_load_template_from_file>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_load_template_from_file>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_load_template_from_file>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_load_template_from_file>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_load_template_from_file>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_load_template_from_file>
            nextitem: <Function test_get_model_from_provider>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_load_template_from_file>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_load_template_from_file' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_load_template_from_file
            location: ('logprob_ranker/tests/test_cli.py', 82, 'test_load_template_from_file')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_get_model_from_provider>
          nextitem: <Function test_on_output_generated>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_get_model_from_provider
            location: ('logprob_ranker/tests/test_cli.py', 106, 'test_get_model_from_provider')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_get_model_from_provider>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_get_model_from_provider>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_get_model_from_provider>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_get_model_from_provider>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_get_model_from_provider>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_get_model_from_provider>
            nextitem: <Function test_on_output_generated>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_get_model_from_provider>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_get_model_from_provider' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_get_model_from_provider
            location: ('logprob_ranker/tests/test_cli.py', 106, 'test_get_model_from_provider')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_on_output_generated>
          nextitem: <Function test_main_rank_command>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_on_output_generated
            location: ('logprob_ranker/tests/test_cli.py', 128, 'test_on_output_generated')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_on_output_generated>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_on_output_generated>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_on_output_generated>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_on_output_generated>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_on_output_generated>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_on_output_generated>
            nextitem: <Function test_main_rank_command>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_on_output_generated>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_on_output_generated' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_on_output_generated
            location: ('logprob_ranker/tests/test_cli.py', 128, 'test_on_output_generated')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_main_rank_command>
          nextitem: <Function test_main_no_command>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_main_rank_command
            location: ('logprob_ranker/tests/test_cli.py', 153, 'test_main_rank_command')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_main_rank_command>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_main_rank_command>>
          finish pytest_fixture_setup --> <MagicMock id='140068706850848'> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_rank_command>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_main_rank_command>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_main_rank_command>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_rank_command>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_main_rank_command>
            nextitem: <Function test_main_no_command>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_main_rank_command>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_rank_command>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_rank_command' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_main_rank_command
            location: ('logprob_ranker/tests/test_cli.py', 153, 'test_main_rank_command')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_main_no_command>
          nextitem: <Function test_print_provider_help>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_main_no_command
            location: ('logprob_ranker/tests/test_cli.py', 174, 'test_main_no_command')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_main_no_command>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_no_command>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_main_no_command>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_main_no_command>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_no_command>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_main_no_command>
            nextitem: <Function test_print_provider_help>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_main_no_command>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_main_no_command' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_main_no_command
            location: ('logprob_ranker/tests/test_cli.py', 174, 'test_main_no_command')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_print_provider_help>
          nextitem: <Function test_run_rank_command_no_output_file>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_print_provider_help
            location: ('logprob_ranker/tests/test_cli.py', 192, 'test_print_provider_help')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_print_provider_help>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_print_provider_help>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_print_provider_help>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_print_provider_help>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_print_provider_help>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_print_provider_help>
            nextitem: <Function test_run_rank_command_no_output_file>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_print_provider_help>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_print_provider_help' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_print_provider_help
            location: ('logprob_ranker/tests/test_cli.py', 192, 'test_print_provider_help')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_rank_command_no_output_file>
          nextitem: <Function test_run_rank_command_with_output_file>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file
            location: ('logprob_ranker/tests/test_cli.py', 208, 'test_run_rank_command_no_output_file')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_rank_command_no_output_file>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_run_rank_command_no_output_file>>
          finish pytest_fixture_setup --> <MagicMock id='140068706815232'> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_no_output_file>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_rank_command_no_output_file>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_rank_command_no_output_file>
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_no_output_file>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_rank_command_no_output_file>
            nextitem: <Function test_run_rank_command_with_output_file>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_run_rank_command_no_output_file>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_no_output_file>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file
            location: ('logprob_ranker/tests/test_cli.py', 208, 'test_run_rank_command_no_output_file')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_no_output_file
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_rank_command_with_output_file>
          nextitem: <TestCaseFunction test_anthropic_integration>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file
            location: ('logprob_ranker/tests/test_cli.py', 249, 'test_run_rank_command_with_output_file')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_rank_command_with_output_file>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_run_rank_command_with_output_file>>
          finish pytest_fixture_setup --> <MagicMock id='140068706390368'> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_with_output_file>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_rank_command_with_output_file>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_rank_command_with_output_file>
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_with_output_file>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_rank_command_with_output_file>
            nextitem: <TestCaseFunction test_anthropic_integration>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='mock_args_rank' scope='function' baseid='logprob_ranker/tests/test_cli.py'>
              request: <SubRequest 'mock_args_rank' for <Function test_run_rank_command_with_output_file>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_rank_command_with_output_file>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file
            location: ('logprob_ranker/tests/test_cli.py', 249, 'test_run_rank_command_with_output_file')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_cli.py::test_run_rank_command_with_output_file
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_anthropic_integration>
          nextitem: <TestCaseFunction test_create_chat_completion>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 376, 'TestLiteLLMAdapter.test_anthropic_integration')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_anthropic_integration>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_anthropic_integration>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_integration>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_anthropic_integration>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_integration>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_anthropic_integration>
            nextitem: <TestCaseFunction test_create_chat_completion>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_integration>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_anthropic_integration
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 376, 'TestLiteLLMAdapter.test_anthropic_integration')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_create_chat_completion>
          nextitem: <TestCaseFunction test_empty_logprobs_content_mocked>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 218, 'TestLiteLLMAdapter.test_create_chat_completion')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_create_chat_completion>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_create_chat_completion>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_create_chat_completion>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_create_chat_completion>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_create_chat_completion>
            nextitem: <TestCaseFunction test_empty_logprobs_content_mocked>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_create_chat_completion>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_create_chat_completion
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 218, 'TestLiteLLMAdapter.test_create_chat_completion')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_empty_logprobs_content_mocked>
          nextitem: <TestCaseFunction test_initialization>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 535, 'TestLiteLLMAdapter.test_empty_logprobs_content_mocked')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
            nextitem: <TestCaseFunction test_initialization>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_empty_logprobs_content_mocked>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_empty_logprobs_content_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 535, 'TestLiteLLMAdapter.test_empty_logprobs_content_mocked')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_initialization>
          nextitem: <TestCaseFunction test_malformed_logprobs_item_mocked>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 182, 'TestLiteLLMAdapter.test_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_initialization>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_initialization>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_initialization>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_initialization>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_initialization>
            nextitem: <TestCaseFunction test_malformed_logprobs_item_mocked>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_initialization>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_initialization
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 182, 'TestLiteLLMAdapter.test_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_malformed_logprobs_item_mocked>
          nextitem: <TestCaseFunction test_missing_logprobs_attribute_mocked>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 599, 'TestLiteLLMAdapter.test_malformed_logprobs_item_mocked')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
            nextitem: <TestCaseFunction test_missing_logprobs_attribute_mocked>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_malformed_logprobs_item_mocked>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_malformed_logprobs_item_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 599, 'TestLiteLLMAdapter.test_malformed_logprobs_item_mocked')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
          nextitem: <TestCaseFunction test_rank_outputs>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 492, 'TestLiteLLMAdapter.test_missing_logprobs_attribute_mocked')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
            nextitem: <TestCaseFunction test_rank_outputs>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_missing_logprobs_attribute_mocked>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_missing_logprobs_attribute_mocked
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 492, 'TestLiteLLMAdapter.test_missing_logprobs_attribute_mocked')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_rank_outputs>
          nextitem: <TestCaseFunction test_real_logprobs_output>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 306, 'TestLiteLLMAdapter.test_rank_outputs')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_rank_outputs>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_rank_outputs>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_rank_outputs>
            nextitem: <TestCaseFunction test_real_logprobs_output>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_rank_outputs
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 306, 'TestLiteLLMAdapter.test_rank_outputs')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_real_logprobs_output>
          nextitem: <TestCaseFunction test_anthropic_initialization>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 465, 'TestLiteLLMAdapter.test_real_logprobs_output')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_real_logprobs_output>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_real_logprobs_output>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_real_logprobs_output>
        matched marked file 'anyio._backends' (from 'anyio') [assertion]
        find_module called for: anyio._backends [assertion]
        found cached rewritten pyc for /home/tom/.local/lib/python3.8/site-packages/anyio/_backends/__init__.py [assertion]
        matched marked file 'anyio._backends._asyncio' (from 'anyio') [assertion]
        find_module called for: anyio._backends._asyncio [assertion]
        found cached rewritten pyc for /home/tom/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py [assertion]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_real_logprobs_output>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_real_logprobs_output>
            nextitem: <TestCaseFunction test_anthropic_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_real_logprobs_output>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_rank_outputs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_missing_logprobs_attribute_mocked>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_malformed_logprobs_item_mocked>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_empty_logprobs_content_mocked>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_create_chat_completion>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMAdapter' scope='class' baseid='logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMAdapter' for <TestCaseFunction test_anthropic_integration>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_real_logprobs_output>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output
            location: ('logprob_ranker/tests/test_litellm_adapter.py', 465, 'TestLiteLLMAdapter.test_real_logprobs_output')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : DeprecationWarning("Use 'content=<...>' to upload raw bytes/text content."), category : 'DeprecationWarning', filename : '/home/tom/.local/lib/python3.8/site-packages/httpx/_models.py', lineno : 408, line : None}
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : DeprecationWarning("Use 'content=<...>' to upload raw bytes/text content."), category : 'DeprecationWarning', filename : '/home/tom/.local/lib/python3.8/site-packages/httpx/_models.py', lineno : 408, line : None}
            nodeid: logprob_ranker/tests/test_litellm_adapter.py::TestLiteLLMAdapter::test_real_logprobs_output
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_anthropic_initialization>
          nextitem: <TestCaseFunction test_custom_initialization>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 42, 'TestLiteLLMBasic.test_anthropic_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_anthropic_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMBasic' scope='class' baseid='logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMBasic' for <TestCaseFunction test_anthropic_initialization>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_initialization>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_anthropic_initialization>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_initialization>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_anthropic_initialization>
            nextitem: <TestCaseFunction test_custom_initialization>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_anthropic_initialization>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_anthropic_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 42, 'TestLiteLLMBasic.test_anthropic_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_custom_initialization>
          nextitem: <TestCaseFunction test_openai_initialization>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 55, 'TestLiteLLMBasic.test_custom_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_custom_initialization>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_custom_initialization>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_custom_initialization>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_custom_initialization>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_custom_initialization>
            nextitem: <TestCaseFunction test_openai_initialization>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_custom_initialization>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_custom_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 55, 'TestLiteLLMBasic.test_custom_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_openai_initialization>
          nextitem: <Function test_simple_rank>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 28, 'TestLiteLLMBasic.test_openai_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_openai_initialization>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_openai_initialization>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_openai_initialization>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_openai_initialization>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_openai_initialization>
            nextitem: <Function test_simple_rank>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMBasic' scope='class' baseid='logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMBasic' for <TestCaseFunction test_openai_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMBasic' scope='class' baseid='logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMBasic' for <TestCaseFunction test_custom_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLiteLLMBasic' scope='class' baseid='logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLiteLLMBasic' for <TestCaseFunction test_anthropic_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_openai_initialization>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_basic.py::TestLiteLLMBasic::test_openai_initialization
            location: ('logprob_ranker/tests/test_litellm_basic.py', 28, 'TestLiteLLMBasic.test_openai_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simple_rank>
          nextitem: <Function test_sync_wrapper>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank
            location: ('logprob_ranker/tests/test_litellm_functionality.py', 31, 'test_simple_rank')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simple_rank>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_litellm_functionality.py'>
              request: <SubRequest 'config' for <Function test_simple_rank>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=50 top_p=1.0 logprobs=False top_logprobs=None num_variants=1 thread_count=1 evaluation_top_logprobs=None template='{"clear": LOGPROB_TRUE, "useful": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simple_rank>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simple_rank>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simple_rank>
        pytest_runtest_makereport [hook]
            item: <Function test_simple_rank>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simple_rank>
            nextitem: <Function test_sync_wrapper>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_litellm_functionality.py'>
              request: <SubRequest 'config' for <Function test_simple_rank>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simple_rank>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank
            location: ('logprob_ranker/tests/test_litellm_functionality.py', 31, 'test_simple_rank')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_litellm_functionality.py::test_simple_rank
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_sync_wrapper>
          nextitem: <TestCaseFunction test_gen_eval_handles_evaluation_error>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper
            location: ('logprob_ranker/tests/test_litellm_functionality.py', 107, 'test_sync_wrapper')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_sync_wrapper>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_litellm_functionality.py'>
              request: <SubRequest 'config' for <Function test_sync_wrapper>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=50 top_p=1.0 logprobs=False top_logprobs=None num_variants=1 thread_count=1 evaluation_top_logprobs=None template='{"clear": LOGPROB_TRUE, "useful": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sync_wrapper>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_sync_wrapper>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_sync_wrapper>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sync_wrapper>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_sync_wrapper>
            nextitem: <TestCaseFunction test_gen_eval_handles_evaluation_error>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_litellm_functionality.py'>
              request: <SubRequest 'config' for <Function test_sync_wrapper>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sync_wrapper>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_litellm_functionality.py::test_sync_wrapper
            location: ('logprob_ranker/tests/test_litellm_functionality.py', 107, 'test_sync_wrapper')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
          nextitem: <TestCaseFunction test_gen_eval_handles_generation_error>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 99, 'TestLogProbRankerErrorHandling.test_gen_eval_handles_evaluation_error')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' for <TestCaseFunction test_gen_eval_handles_evaluation_error>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
            nextitem: <TestCaseFunction test_gen_eval_handles_generation_error>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_evaluation_error>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_evaluation_error
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 99, 'TestLogProbRankerErrorHandling.test_gen_eval_handles_evaluation_error')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_gen_eval_handles_generation_error>
          nextitem: <TestCaseFunction test_rank_outputs_handles_all_failures>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 76, 'TestLogProbRankerErrorHandling.test_gen_eval_handles_generation_error')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
            nextitem: <TestCaseFunction test_rank_outputs_handles_all_failures>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_gen_eval_handles_generation_error>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_gen_eval_handles_generation_error
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 76, 'TestLogProbRankerErrorHandling.test_gen_eval_handles_generation_error')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_rank_outputs_handles_all_failures>
          nextitem: <TestCaseFunction test_rank_outputs_handles_some_failures>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 138, 'TestLogProbRankerErrorHandling.test_rank_outputs_handles_all_failures')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
            nextitem: <TestCaseFunction test_rank_outputs_handles_some_failures>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_all_failures>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_all_failures
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 138, 'TestLogProbRankerErrorHandling.test_rank_outputs_handles_all_failures')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_rank_outputs_handles_some_failures>
          nextitem: <TestCaseFunction test_simple_successful_scoring>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 124, 'TestLogProbRankerErrorHandling.test_rank_outputs_handles_some_failures')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
            nextitem: <TestCaseFunction test_simple_successful_scoring>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' for <TestCaseFunction test_rank_outputs_handles_some_failures>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' for <TestCaseFunction test_rank_outputs_handles_all_failures>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' for <TestCaseFunction test_gen_eval_handles_generation_error>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerErrorHandling' for <TestCaseFunction test_gen_eval_handles_evaluation_error>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_rank_outputs_handles_some_failures>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerErrorHandling::test_rank_outputs_handles_some_failures
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 124, 'TestLogProbRankerErrorHandling.test_rank_outputs_handles_some_failures')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_simple_successful_scoring>
          nextitem: <Function test_initialization>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 212, 'TestLogProbRankerScoring.test_simple_successful_scoring')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerScoring' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerScoring' for <TestCaseFunction test_simple_successful_scoring>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <TestCaseFunction test_simple_successful_scoring>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('0.0 != -0.6333333333333333 within 5 places (0.6333333333333333 difference)') tblen=4>>
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
            nextitem: <Function test_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestLogProbRankerScoring' scope='class' baseid='logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestLogProbRankerScoring' for <TestCaseFunction test_simple_successful_scoring>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_simple_successful_scoring>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring
            location: ('logprob_ranker/tests/test_logprob_ranker.py', 212, 'TestLogProbRankerScoring.test_simple_successful_scoring')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_initialization>
          nextitem: <Function test_initialization_default_config>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization
            location: ('logprob_ranker/tests/test_ranker.py', 77, 'test_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_initialization>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=1000 top_p=1.0 logprobs=False top_logprobs=None num_variants=2 thread_count=1 evaluation_top_logprobs=None template='{"test": LOGPROB_TRUE, "quality": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_initialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_initialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_initialization>
            nextitem: <Function test_initialization_default_config>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization
            location: ('logprob_ranker/tests/test_ranker.py', 77, 'test_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_initialization_default_config>
          nextitem: <Function test_initialization_with_callback>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization_default_config
            location: ('logprob_ranker/tests/test_ranker.py', 83, 'test_initialization_default_config')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_initialization_default_config>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_default_config>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_initialization_default_config>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_initialization_default_config>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_default_config>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_initialization_default_config>
            nextitem: <Function test_initialization_with_callback>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_default_config>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_default_config' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization_default_config
            location: ('logprob_ranker/tests/test_ranker.py', 83, 'test_initialization_default_config')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_initialization_with_callback>
          nextitem: <Function test_parse_evaluation>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization_with_callback
            location: ('logprob_ranker/tests/test_ranker.py', 92, 'test_initialization_with_callback')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_initialization_with_callback>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_with_callback>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_initialization_with_callback>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_initialization_with_callback>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_with_callback>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_initialization_with_callback>
            nextitem: <Function test_parse_evaluation>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_initialization_with_callback>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_initialization_with_callback' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_initialization_with_callback
            location: ('logprob_ranker/tests/test_ranker.py', 92, 'test_initialization_with_callback')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parse_evaluation>
          nextitem: <Function test_generate_and_evaluate_output>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_parse_evaluation
            location: ('logprob_ranker/tests/test_ranker.py', 99, 'test_parse_evaluation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parse_evaluation>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parse_evaluation>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parse_evaluation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parse_evaluation>
        pytest_runtest_makereport [hook]
            item: <Function test_parse_evaluation>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parse_evaluation>
            nextitem: <Function test_generate_and_evaluate_output>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parse_evaluation>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_parse_evaluation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_parse_evaluation
            location: ('logprob_ranker/tests/test_ranker.py', 99, 'test_parse_evaluation')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_ranker.py::test_parse_evaluation
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_generate_and_evaluate_output>
          nextitem: <Function test_generate_and_evaluate_output_failure>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output
            location: ('logprob_ranker/tests/test_ranker.py', 139, 'test_generate_and_evaluate_output')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_generate_and_evaluate_output>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_generate_and_evaluate_output>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=1000 top_p=1.0 logprobs=False top_logprobs=None num_variants=2 thread_count=1 evaluation_top_logprobs=None template='{"test": LOGPROB_TRUE, "quality": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_generate_and_evaluate_output>>
          finish pytest_fixture_setup --> <logprob_ranker.tests.test_ranker.ConcreteTestRanker object at 0x7f6448e5b880> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_generate_and_evaluate_output>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_generate_and_evaluate_output>
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_generate_and_evaluate_output>
            nextitem: <Function test_generate_and_evaluate_output_failure>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_generate_and_evaluate_output>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_generate_and_evaluate_output>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_generate_and_evaluate_output>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output
            location: ('logprob_ranker/tests/test_ranker.py', 139, 'test_generate_and_evaluate_output')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_generate_and_evaluate_output_failure>
          nextitem: <Function test_rank_outputs>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure
            location: ('logprob_ranker/tests/test_ranker.py', 185, 'test_generate_and_evaluate_output_failure')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_generate_and_evaluate_output_failure>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output_failure>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_generate_and_evaluate_output_failure>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_generate_and_evaluate_output_failure>
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output_failure>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_generate_and_evaluate_output_failure>
            nextitem: <Function test_rank_outputs>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_generate_and_evaluate_output_failure>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure
            location: ('logprob_ranker/tests/test_ranker.py', 185, 'test_generate_and_evaluate_output_failure')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_ranker.py::test_generate_and_evaluate_output_failure
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_rank_outputs>
          nextitem: <Function test_rank_outputs_sync>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_rank_outputs
            location: ('logprob_ranker/tests/test_ranker.py', 213, 'test_rank_outputs')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_rank_outputs>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_rank_outputs>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=1000 top_p=1.0 logprobs=False top_logprobs=None num_variants=2 thread_count=1 evaluation_top_logprobs=None template='{"test": LOGPROB_TRUE, "quality": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs>>
          finish pytest_fixture_setup --> <logprob_ranker.tests.test_ranker.ConcreteTestRanker object at 0x7f6448e74be0> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_rank_outputs>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_rank_outputs>
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs>
            call: <CallInfo when='call' excinfo=<ExceptionInfo async def function and no async plugin installed (see warnings) tblen=27>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='call' outcome='skipped'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='call' outcome='skipped'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='call' outcome='skipped'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('skipped', 's', 'SKIPPED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_rank_outputs>
            nextitem: <Function test_rank_outputs_sync>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_rank_outputs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_rank_outputs
            location: ('logprob_ranker/tests/test_ranker.py', 213, 'test_rank_outputs')
        finish pytest_runtest_logfinish --> [] [hook]
        pytest_warning_recorded [hook]
            warning_message: {message : PytestUnhandledCoroutineWarning('async def functions are not natively supported and have been skipped.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted'), category : 'PytestUnhandledCoroutineWarning', filename : '/usr/local/lib/python3.8/dist-packages/_pytest/python.py', lineno : 183, line : None}
            nodeid: logprob_ranker/tests/test_ranker.py::test_rank_outputs
            when: runtest
            location: None
        finish pytest_warning_recorded --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_rank_outputs_sync>
          nextitem: <Function test_sort_ranked_outputs>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync
            location: ('logprob_ranker/tests/test_ranker.py', 258, 'test_rank_outputs_sync')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_rank_outputs_sync>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_rank_outputs_sync>>
          finish pytest_fixture_setup --> temperature=0.7 max_tokens=1000 top_p=1.0 logprobs=False top_logprobs=None num_variants=2 thread_count=1 evaluation_top_logprobs=None template='{"test": LOGPROB_TRUE, "quality": LOGPROB_TRUE}' system_prompt='You are a creative assistant that provides a single concise response.' evaluation_prompt='You are an evaluator. Evaluate the following text based on the criteria.\nReturn ONLY a JSON object with your evaluation. Use JSON boolean values (true/false).' [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs_sync>>
          finish pytest_fixture_setup --> <logprob_ranker.tests.test_ranker.ConcreteTestRanker object at 0x7f6448e743a0> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs_sync>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_rank_outputs_sync>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_rank_outputs_sync>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x7f645106a280>
                op: ==
                left: -0.7
                right: -0.44999999999999996 ± 4.5e-07
            finish pytest_assertrepr_compare --> [['-0.7 == -0.44999999999999996 ± 4.5e-07', 'comparison failed', 'Obtained: -0.7', 'Expected: -0.44999999999999996 ± 4.5e-07']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs_sync>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert -0.7 == -0.44999999999999996 ± 4.5e-07\n  comparison failed\n  Obtained: -0.7\n  Expected: -0.44999999999999996 ± 4.5e-07') tblen=28>>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_rank_outputs_sync>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert -0.7 == -0.44999999999999996 ± 4.5e-07\n  comparison failed\n  Obtained: -0.7\n  Expected: -0.44999999999999996 ± 4.5e-07') tblen=1>>
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_rank_outputs_sync>
            nextitem: <Function test_sort_ranked_outputs>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs_sync>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ranker' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'ranker' for <Function test_rank_outputs_sync>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='config' scope='function' baseid='logprob_ranker/tests/test_ranker.py'>
              request: <SubRequest 'config' for <Function test_rank_outputs_sync>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_rank_outputs_sync>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync
            location: ('logprob_ranker/tests/test_ranker.py', 258, 'test_rank_outputs_sync')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_sort_ranked_outputs>
          nextitem: <TestCaseFunction test_parse_evaluation_json_direct>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs
            location: ('logprob_ranker/tests/test_ranker.py', 311, 'test_sort_ranked_outputs')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_sort_ranked_outputs>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sort_ranked_outputs>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_sort_ranked_outputs>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_sort_ranked_outputs>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sort_ranked_outputs>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_sort_ranked_outputs>
            nextitem: <TestCaseFunction test_parse_evaluation_json_direct>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_sort_ranked_outputs>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_ranker.py::test_sort_ranked_outputs
            location: ('logprob_ranker/tests/test_ranker.py', 311, 'test_sort_ranked_outputs')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_direct>
          nextitem: <TestCaseFunction test_parse_evaluation_json_in_markdown>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct
            location: ('logprob_ranker/tests/test_utils.py', 20, 'TestParseEvaluationJson.test_parse_evaluation_json_direct')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_direct>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
            nextitem: <TestCaseFunction test_parse_evaluation_json_in_markdown>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_direct>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_direct
            location: ('logprob_ranker/tests/test_utils.py', 20, 'TestParseEvaluationJson.test_parse_evaluation_json_direct')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
          nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown
            location: ('logprob_ranker/tests/test_utils.py', 25, 'TestParseEvaluationJson.test_parse_evaluation_json_in_markdown')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
            nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_in_markdown>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_in_markdown
            location: ('logprob_ranker/tests/test_utils.py', 25, 'TestParseEvaluationJson.test_parse_evaluation_json_in_markdown')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
          nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json
            location: ('logprob_ranker/tests/test_utils.py', 35, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
            nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json
            location: ('logprob_ranker/tests/test_utils.py', 35, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
          nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown
            location: ('logprob_ranker/tests/test_utils.py', 40, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json_in_markdown')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
            nextitem: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_in_markdown
            location: ('logprob_ranker/tests/test_utils.py', 40, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json_in_markdown')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
          nextitem: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text
            location: ('logprob_ranker/tests/test_utils.py', 45, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json_with_text')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
            nextitem: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_invalid_json_with_text
            location: ('logprob_ranker/tests/test_utils.py', 45, 'TestParseEvaluationJson.test_parse_evaluation_json_invalid_json_with_text')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
          nextitem: <TestCaseFunction test_parse_evaluation_json_no_json>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects
            location: ('logprob_ranker/tests/test_utils.py', 55, 'TestParseEvaluationJson.test_parse_evaluation_json_multiple_json_objects')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
            nextitem: <TestCaseFunction test_parse_evaluation_json_no_json>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_multiple_json_objects
            location: ('logprob_ranker/tests/test_utils.py', 55, 'TestParseEvaluationJson.test_parse_evaluation_json_multiple_json_objects')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_no_json>
          nextitem: <TestCaseFunction test_parse_evaluation_json_not_dict>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json
            location: ('logprob_ranker/tests/test_utils.py', 50, 'TestParseEvaluationJson.test_parse_evaluation_json_no_json')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
            nextitem: <TestCaseFunction test_parse_evaluation_json_not_dict>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_no_json>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_no_json
            location: ('logprob_ranker/tests/test_utils.py', 50, 'TestParseEvaluationJson.test_parse_evaluation_json_no_json')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_not_dict>
          nextitem: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict
            location: ('logprob_ranker/tests/test_utils.py', 61, 'TestParseEvaluationJson.test_parse_evaluation_json_not_dict')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
            nextitem: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict
            location: ('logprob_ranker/tests/test_utils.py', 61, 'TestParseEvaluationJson.test_parse_evaluation_json_not_dict')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
          nextitem: <TestCaseFunction test_parse_evaluation_json_with_text>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text
            location: ('logprob_ranker/tests/test_utils.py', 66, 'TestParseEvaluationJson.test_parse_evaluation_json_not_dict_in_text')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
            nextitem: <TestCaseFunction test_parse_evaluation_json_with_text>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_not_dict_in_text
            location: ('logprob_ranker/tests/test_utils.py', 66, 'TestParseEvaluationJson.test_parse_evaluation_json_not_dict_in_text')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_parse_evaluation_json_with_text>
          nextitem: <TestCaseFunction test_deserialize_ranked_output_basic>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text
            location: ('logprob_ranker/tests/test_utils.py', 30, 'TestParseEvaluationJson.test_parse_evaluation_json_with_text')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
            nextitem: <TestCaseFunction test_deserialize_ranked_output_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_with_text>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_not_dict_in_text>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_not_dict>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_no_json>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_multiple_json_objects>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_invalid_json_with_text>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_invalid_json_in_markdown>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_invalid_json>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_in_markdown>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestParseEvaluationJson' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestParseEvaluationJson'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestParseEvaluationJson' for <TestCaseFunction test_parse_evaluation_json_direct>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_parse_evaluation_json_with_text>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestParseEvaluationJson::test_parse_evaluation_json_with_text
            location: ('logprob_ranker/tests/test_utils.py', 30, 'TestParseEvaluationJson.test_parse_evaluation_json_with_text')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_basic>
          nextitem: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic
            location: ('logprob_ranker/tests/test_utils.py', 211, 'TestUtils.test_deserialize_ranked_output_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_deserialize_ranked_output_basic>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
            nextitem: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_basic>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_basic
            location: ('logprob_ranker/tests/test_utils.py', 211, 'TestUtils.test_deserialize_ranked_output_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
          nextitem: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types
            location: ('logprob_ranker/tests/test_utils.py', 256, 'TestUtils.test_deserialize_ranked_output_incorrect_types')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
            nextitem: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_incorrect_types>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_incorrect_types
            location: ('logprob_ranker/tests/test_utils.py', 256, 'TestUtils.test_deserialize_ranked_output_incorrect_types')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
          nextitem: <TestCaseFunction test_deserialize_ranked_output_with_extras>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields
            location: ('logprob_ranker/tests/test_utils.py', 240, 'TestUtils.test_deserialize_ranked_output_missing_required_fields')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
            nextitem: <TestCaseFunction test_deserialize_ranked_output_with_extras>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_missing_required_fields
            location: ('logprob_ranker/tests/test_utils.py', 240, 'TestUtils.test_deserialize_ranked_output_missing_required_fields')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
          nextitem: <TestCaseFunction test_extract_template_attributes_basic>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras
            location: ('logprob_ranker/tests/test_utils.py', 221, 'TestUtils.test_deserialize_ranked_output_with_extras')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
            nextitem: <TestCaseFunction test_extract_template_attributes_basic>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_deserialize_ranked_output_with_extras>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_deserialize_ranked_output_with_extras
            location: ('logprob_ranker/tests/test_utils.py', 221, 'TestUtils.test_deserialize_ranked_output_with_extras')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_basic>
          nextitem: <TestCaseFunction test_extract_template_attributes_case_insensitive>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic
            location: ('logprob_ranker/tests/test_utils.py', 74, 'TestUtils.test_extract_template_attributes_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
            nextitem: <TestCaseFunction test_extract_template_attributes_case_insensitive>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_basic>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_basic
            location: ('logprob_ranker/tests/test_utils.py', 74, 'TestUtils.test_extract_template_attributes_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
          nextitem: <TestCaseFunction test_extract_template_attributes_empty_json>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive
            location: ('logprob_ranker/tests/test_utils.py', 117, 'TestUtils.test_extract_template_attributes_case_insensitive')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
            nextitem: <TestCaseFunction test_extract_template_attributes_empty_json>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_case_insensitive>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_case_insensitive
            location: ('logprob_ranker/tests/test_utils.py', 117, 'TestUtils.test_extract_template_attributes_case_insensitive')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_empty_json>
          nextitem: <TestCaseFunction test_extract_template_attributes_invalid_json>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json
            location: ('logprob_ranker/tests/test_utils.py', 89, 'TestUtils.test_extract_template_attributes_empty_json')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
            nextitem: <TestCaseFunction test_extract_template_attributes_invalid_json>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_empty_json>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_empty_json
            location: ('logprob_ranker/tests/test_utils.py', 89, 'TestUtils.test_extract_template_attributes_empty_json')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_invalid_json>
          nextitem: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json
            location: ('logprob_ranker/tests/test_utils.py', 94, 'TestUtils.test_extract_template_attributes_invalid_json')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
            nextitem: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json
            location: ('logprob_ranker/tests/test_utils.py', 94, 'TestUtils.test_extract_template_attributes_invalid_json')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
          nextitem: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match
            location: ('logprob_ranker/tests/test_utils.py', 131, 'TestUtils.test_extract_template_attributes_invalid_json_no_match')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('"No LOGPROB_TRUE attributes found" does not match "No attributes (e.g., "my_attribute": LOGPROB_TRUE) found in template: \'{key: LOGPROB_TRUE}\'. Ensure attributes are quoted strings followed by a LOGPROB_X placeholder."') tblen=1>>
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
            nextitem: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match
            location: ('logprob_ranker/tests/test_utils.py', 131, 'TestUtils.test_extract_template_attributes_invalid_json_no_match')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
          nextitem: <TestCaseFunction test_extract_template_attributes_mixed_values>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring
            location: ('logprob_ranker/tests/test_utils.py', 124, 'TestUtils.test_extract_template_attributes_logprob_as_key_or_substring')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
            nextitem: <TestCaseFunction test_extract_template_attributes_mixed_values>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_logprob_as_key_or_substring
            location: ('logprob_ranker/tests/test_utils.py', 124, 'TestUtils.test_extract_template_attributes_logprob_as_key_or_substring')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_mixed_values>
          nextitem: <TestCaseFunction test_extract_template_attributes_no_logprob>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values
            location: ('logprob_ranker/tests/test_utils.py', 79, 'TestUtils.test_extract_template_attributes_mixed_values')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
            nextitem: <TestCaseFunction test_extract_template_attributes_no_logprob>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_mixed_values>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_mixed_values
            location: ('logprob_ranker/tests/test_utils.py', 79, 'TestUtils.test_extract_template_attributes_mixed_values')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_no_logprob>
          nextitem: <TestCaseFunction test_extract_template_attributes_regex_fallback>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob
            location: ('logprob_ranker/tests/test_utils.py', 84, 'TestUtils.test_extract_template_attributes_no_logprob')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
            nextitem: <TestCaseFunction test_extract_template_attributes_regex_fallback>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_no_logprob>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_no_logprob
            location: ('logprob_ranker/tests/test_utils.py', 84, 'TestUtils.test_extract_template_attributes_no_logprob')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
          nextitem: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback
            location: ('logprob_ranker/tests/test_utils.py', 100, 'TestUtils.test_extract_template_attributes_regex_fallback')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
            nextitem: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_regex_fallback>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_regex_fallback
            location: ('logprob_ranker/tests/test_utils.py', 100, 'TestUtils.test_extract_template_attributes_regex_fallback')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
          nextitem: <TestCaseFunction test_format_evaluation_prompt>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation
            location: ('logprob_ranker/tests/test_utils.py', 111, 'TestUtils.test_extract_template_attributes_whitespace_variation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("Lists differ: ['attr2'] != ['attr1', 'attr2']\n\nFirst differing element 0:\n'attr2'\n'attr1'\n\nSecond list contains 1 additional elements.\nFirst extra element 1:\n'attr2'\n\n- ['attr2']\n+ ['attr1', 'attr2']") tblen=1>>
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
            nextitem: <TestCaseFunction test_format_evaluation_prompt>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_extract_template_attributes_whitespace_variation>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation
            location: ('logprob_ranker/tests/test_utils.py', 111, 'TestUtils.test_extract_template_attributes_whitespace_variation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_format_evaluation_prompt>
          nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt
            location: ('logprob_ranker/tests/test_utils.py', 138, 'TestUtils.test_format_evaluation_prompt')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
            nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt
            location: ('logprob_ranker/tests/test_utils.py', 138, 'TestUtils.test_format_evaluation_prompt')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
          nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt
            location: ('logprob_ranker/tests/test_utils.py', 151, 'TestUtils.test_format_evaluation_prompt_empty_eval_prompt')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
            nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_eval_prompt
            location: ('logprob_ranker/tests/test_utils.py', 151, 'TestUtils.test_format_evaluation_prompt_empty_eval_prompt')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
          nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_template>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text
            location: ('logprob_ranker/tests/test_utils.py', 155, 'TestUtils.test_format_evaluation_prompt_empty_generated_text')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
            nextitem: <TestCaseFunction test_format_evaluation_prompt_empty_template>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_generated_text
            location: ('logprob_ranker/tests/test_utils.py', 155, 'TestUtils.test_format_evaluation_prompt_empty_generated_text')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
          nextitem: <TestCaseFunction test_serialize_ranked_output_basic>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template
            location: ('logprob_ranker/tests/test_utils.py', 159, 'TestUtils.test_format_evaluation_prompt_empty_template')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
            nextitem: <TestCaseFunction test_serialize_ranked_output_basic>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_format_evaluation_prompt_empty_template>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_format_evaluation_prompt_empty_template
            location: ('logprob_ranker/tests/test_utils.py', 159, 'TestUtils.test_format_evaluation_prompt_empty_template')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_serialize_ranked_output_basic>
          nextitem: <TestCaseFunction test_serialize_ranked_output_with_extras>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic
            location: ('logprob_ranker/tests/test_utils.py', 186, 'TestUtils.test_serialize_ranked_output_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
            nextitem: <TestCaseFunction test_serialize_ranked_output_with_extras>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_basic>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_basic
            location: ('logprob_ranker/tests/test_utils.py', 186, 'TestUtils.test_serialize_ranked_output_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_serialize_ranked_output_with_extras>
          nextitem: <TestCaseFunction test_sort_ranked_outputs>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras
            location: ('logprob_ranker/tests/test_utils.py', 192, 'TestUtils.test_serialize_ranked_output_with_extras')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
            nextitem: <TestCaseFunction test_sort_ranked_outputs>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_serialize_ranked_output_with_extras>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_serialize_ranked_output_with_extras
            location: ('logprob_ranker/tests/test_utils.py', 192, 'TestUtils.test_serialize_ranked_output_with_extras')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_sort_ranked_outputs>
          nextitem: <TestCaseFunction test_sort_ranked_outputs_empty_list>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs
            location: ('logprob_ranker/tests/test_utils.py', 163, 'TestUtils.test_sort_ranked_outputs')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
            nextitem: <TestCaseFunction test_sort_ranked_outputs_empty_list>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs
            location: ('logprob_ranker/tests/test_utils.py', 163, 'TestUtils.test_sort_ranked_outputs')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
          nextitem: <TestCaseFunction test_sort_ranked_outputs_single_item>
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list
            location: ('logprob_ranker/tests/test_utils.py', 174, 'TestUtils.test_sort_ranked_outputs_empty_list')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
            nextitem: <TestCaseFunction test_sort_ranked_outputs_single_item>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_empty_list>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_empty_list
            location: ('logprob_ranker/tests/test_utils.py', 174, 'TestUtils.test_sort_ranked_outputs_empty_list')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <TestCaseFunction test_sort_ranked_outputs_single_item>
          nextitem: None
        pytest_runtest_logstart [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item
            location: ('logprob_ranker/tests/test_utils.py', 179, 'TestUtils.test_sort_ranked_outputs_single_item')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
            nextitem: None
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_sort_ranked_outputs_single_item>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_sort_ranked_outputs_empty_list>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_sort_ranked_outputs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_serialize_ranked_output_with_extras>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_serialize_ranked_output_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_format_evaluation_prompt_empty_template>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_format_evaluation_prompt_empty_generated_text>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_format_evaluation_prompt_empty_eval_prompt>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_format_evaluation_prompt>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_whitespace_variation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_regex_fallback>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_no_logprob>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_mixed_values>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_logprob_as_key_or_substring>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_invalid_json_no_match>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_invalid_json>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_empty_json>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_case_insensitive>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_extract_template_attributes_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_deserialize_ranked_output_with_extras>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_deserialize_ranked_output_missing_required_fields>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_deserialize_ranked_output_incorrect_types>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_unittest_setUpClass_fixture_TestUtils' scope='class' baseid='logprob_ranker/tests/test_utils.py::TestUtils'>
              request: <SubRequest '_unittest_setUpClass_fixture_TestUtils' for <TestCaseFunction test_deserialize_ranked_output_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <TestCaseFunction test_sort_ranked_outputs_single_item>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x7f645106a280>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: logprob_ranker/tests/test_utils.py::TestUtils::test_sort_ranked_outputs_single_item
            location: ('logprob_ranker/tests/test_utils.py', 179, 'TestUtils.test_sort_ranked_outputs_single_item')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
    finish pytest_runtestloop --> True [hook]
    pytest_sessionfinish [hook]
        session: <Session logprob-ranker exitstatus=<ExitCode.TESTS_FAILED: 1> testsfailed=4 testscollected=72>
        exitstatus: ExitCode.TESTS_FAILED
      pytest_terminal_summary [hook]
          terminalreporter: <_pytest.terminal.TerminalReporter object at 0x7f6450abb850>
          exitstatus: ExitCode.TESTS_FAILED
          config: <_pytest.config.Config object at 0x7f645106a280>
        pytest_report_teststatus [hook]
            report: <TestReport 'logprob_ranker/tests/test_logprob_ranker.py::TestLogProbRankerScoring::test_simple_successful_scoring' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x7f645106a280>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'logprob_ranker/tests/test_ranker.py::test_rank_outputs_sync' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x7f645106a280>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_invalid_json_no_match' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x7f645106a280>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'logprob_ranker/tests/test_utils.py::TestUtils::test_extract_template_attributes_whitespace_variation' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x7f645106a280>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
      finish pytest_terminal_summary --> [] [hook]
    finish pytest_sessionfinish --> [] [hook]
    pytest_unconfigure [hook]
        config: <_pytest.config.Config object at 0x7f645106a280>
    finish pytest_unconfigure --> [] [hook]
